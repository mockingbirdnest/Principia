\documentclass[10pt, a4paper, twoside]{basestyle}

\usepackage{tikz}
\usetikzlibrary{cd}

\usepackage[Mathematics]{semtex}
\usepackage{chngcntr}
\counterwithout{equation}{section}

%%%% Shorthands.
\DeclareMathOperator{\bias}{\mathit{bias}}
\DeclareMathOperator{\ULP}{\mathfrak u}
\DeclareMathOperator{\mant}{\mathfrak m}
\DeclareMathOperator{\expn}{\mathfrak e}
\DeclareMathOperator{\truncate}{\StandardSymbol{Tr}}
\DeclareMathOperator{\twosum}{\StandardSymbol{TwoSum}}
\DeclareMathOperator{\quicktwosum}{\StandardSymbol{QuickTwoSum}}
\DeclareMathOperator{\longadd}{\StandardSymbol{LongAdd}}
\DeclareMathOperator{\twodifference}{\StandardSymbol{TwoDifference}}
\DeclareMathOperator{\quicktwodifference}{\StandardSymbol{QuickTwoDifference}}
\DeclareMathOperator{\longsub}{\StandardSymbol{LongSub}}

% Rounding brackets will be heavily nested, and reading the nesting depth is critically important,
% so we make them grow for readability.
\newcommand{\round}[1]{\doubleSquareBrackets*{#1}}
\newcommand{\roundTowardZero}[1]{\doubleSquareBrackets{#1}_0}
\newcommand{\roundTowardPositive}[1]{\doubleSquareBrackets{#1}_+}
\newcommand{\roundTowardNegative}[1]{\doubleSquareBrackets{#1}_-}
\newcommand{\roundAll}[1]{\doubleSquareBrackets{#1}_⋯}
\newcommand{\hex}[1]{{_{16}}\mathrm{#1}}
\newcommand{\bin}[1]{{_{2}}\mathrm{#1}}
\newcommand{\red}[1]{\tilde{#1}}

%%%% Title and authors.

\title{An Implementation of Sin and Cos Using Gal's Accurate Tables}
\date{\printdate{2025-02-02}}
\author{Pascal~Leroy (phl)}
\begin{document}
\maketitle
\begin{sloppypar}
\noindent
This document describes the implementation of functions \texttt{Sin} and \texttt{Cos} in Principia.  The goals of that implementation are to be portable (including to machines that do not have a fused multiply-add instruction), achieve good performance, and ensure correct rounding.
\end{sloppypar}

\section*{Overview}
The implementation follows the ideas described by \cite{GalBachelis1991} and uses accurate tables produced by the method presented in \cite{StehléZimmermann2005}.  It guarantees correct rounding with a high probability.  In circumstances where it cannot guarantee correct rounding, it falls back to the (slower but correct) implementation provided by the CORE-MATH project \cite{SibidanovZimmermannGlondu2022} \cite{ZimmermannSibidanovGlondu2024}.  More precisely, the algorithm proceeds through the following steps:
\begin{itemize}[nosep]
\item perform argument reduction using Cody and Waite's algorithm in double precision (see \cite[379]{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010});
\item if argument reduction loses too many bits (i.e., the argument is close to a multiple of $\frac{\Pi}{2}$), fall back to \texttt{cr\_sin} or \texttt{cr\_cos};
\item otherwise, uses accurate tables and a polynomial approximation to compute \texttt{Sin} or \texttt{Cos} with extra accuracy;
\item if the result has a ``dangerous rounding configuration'' (as defined by \cite{GalBachelis1991}), fall back to \texttt{cr\_sin} or \texttt{cr\_cos};
\item otherwise return the rounded result of the preceding computation.
\end{itemize}

\section*{Notation and Accuracy Model} 
In this document we assume a base-2 floating-point number system with $M$ significand bits\footnote{In \texttt{binary64}, $M = 53$.} similar to the IEEE formats.  We define a real  function $\mant$ and an integer function $\expn$ denoting the \emph{significand} and \emph{exponent} of a real number, respectively:
\[
x = ±\mant\of{x} \times 2^{\expn\of{x}} \qquad\text{with}\qquad 2^{M-1} \leq \mant\of{x} \leq 2^M - 1
\]
Note that this representation is unique.  Furthermore, if $x$ is a floating-point number, $\mant\of{x}$ is an integer.

The \emph{unit of the last place} of $x$ is defined as:
\[
\ULP\of{x} \DefineAs 2^{\expn\of{x}}
\]
In particular, $\ULP\of{1} = 2^{1 - M}$ and:
\begin{equation}
\frac{\abs x}{2^M} < \frac{\abs x}{2^M - 1} \leq \ULP\of{x} \leq \frac{\abs x}{2^{M - 1}}
\label{eqnulp}
\end{equation}

We ignore the exponent bias, overflow and underflow as they play no role in this discussion.

Finally, for error analysis we use the accuracy model of \cite{Higham2002}, equation (2.4): everywhere they appear, the quantities $\gd_i$ represent a roundoff factor such that $\abs{\gd_i} < u = 2^{-M}$ (see pages 37 and 38).  We also use $\gq_n$ and $\gg_n$ with the same meaning as in \cite{Higham2002}, lemma 3.1.

\section*{Approximation of $\frac{\Pi}{2}$}
To perform argument reduction, we need to build approximations of $\frac{\Pi}{2}$ with extra accuracy and analyse the circumstances under which they may be used and the errors that they entail on the reduced argument.

Let $z \geq 0$.  We start by defining the truncation function $\truncate\of{\gk, z}$ which clears the last $\gk$ bits of the significand of $z$:
\[
\truncate\of{\gk, z} \DefineAs \floor{2^{-\gk} \mant \of{z}} \; 2^{\gk} \ULP\of{z}
\]
We have:
\[
z - \truncate\of{\gk, z} = \pa{2^{-\gk} \mant \of{z} - \floor{2^{-\gk} \mant \of{z}}} \; 2^{\gk} \ULP\of{z}
\]
The definition of the floor function implies that the quantity in parentheses is in $\intclop 0 1$ and therefore:
\[
0 \leq z - \truncate\of{\gk, z} < 2^{\gk} \ULP\of{z}
\]
Furthermore if the bits that are being truncated start with exactly $k$ zeros we have the stricter inequality:
\begin{equation}
2^{\gk' - 1} \ULP\of{z} \leq z - \truncate\of{\gk, z} < 2^{\gk'} \ULP\of{z} \quad \text{with} \quad \gk' = \gk - k
\label{eqntruncerror}
\end{equation}
This leads to the following upper bound for the unit of the last place of the truncation error:
\[
\ULP\of{z - \truncate\of{\gk, z}} < 2^{\gk' - M + 1} \ULP\of{z}
\]
which can be made more precise by noting that the function $\ULP$ is always a power of $2$:
\begin{equation}
\ULP\of{z - \truncate\of{\gk, z}} = 2^{\gk' - M} \ULP\of{z}
\label{eqnulptr}
\end{equation}

\subsubsection*{Two-Term Approximation}
In this scheme we approximate $\frac{\Pi}{2}$ as the sum of two floating-point numbers:
\[
\frac{\Pi}{2} \simeq C_1 + \gd C_1
\]
which are defined as:
\begin{equation*}
\begin{dcases}
C_1 &\DefineAs \truncate\of{\gk_1, \frac{\Pi}{2}} \\
\gd C_1 &\DefineAs \round{\frac{\Pi}{2} - C_1}
\end{dcases}
\end{equation*}
Equation (\ref{eqntruncerror}) applied to the definition of $C_1$ yields:
\[
2^{\gk'_1 - 1} \ULP\of{\frac{\Pi}{2}} \leq \frac{\Pi}{2} - C_1 < 2^{\gk'_1} \ULP\of{\frac{\Pi}{2}}
\]
where $\gk'_1 \leq \gk_1$ accounts for any leading zeroes in the bits of $\frac{\Pi}{2}$ that are being truncated.  Accordingly equation (\ref{eqnulptr}) yields, for the unit of the last place:
\[
\ULP\of{\frac{\Pi}{2} - C_1} = 2^{\gk'_1 - M} \ULP\of{\frac{\Pi}{2}}
\]

Noting that the absolute error on the rounding that appears in the definition of $\gd C_1$ is bounded by $\frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_1}$, we obtain the absolute error on the two-term approximation:
\begin{align}
\abs{\frac{\Pi}{2} - C_1 - \gd C_1} \leq \frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_1} = 2^{\gk'_1 - M - 1} \ULP\of{\frac{\Pi}{2}}
\label{eqnpitwoterms}
\end{align}
and the following upper bound for $\gd C_1$:
\begin{align}
\abs{\gd C_1} &< \frac{\Pi}{2} - C_1 + \frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_1} \nonumber \\
&< 2^{\gk'_1} \ULP\of{\frac{\Pi}{2}}+2^{\gk'_1 - M - 1} \ULP\of{\frac{\Pi}{2}} = 2^{\gk'_1} \pa{1 + 2^{-M - 1}} \ULP\of{\frac{\Pi}{2}}
\label{eqnabsdc1}
\end{align}
 
This scheme gives a representation with a significand that has effectively $2 M - \gk'_1$ bits and is such that multiplying $C_1$ by an integer less than or equal to $2^{\gk_1}$ is exact.

\subsubsection*{Three-Term Approximation}
In this scheme we approximate $\frac{\Pi}{2}$ as the sum of three floating-point numbers:
\[
\frac{\Pi}{2} \simeq C_2 + C'_2 + \gd C_2
\]
which are defined as:
\begin{equation*}
\begin{dcases}
C_2 &\DefineAs \truncate\of{\gk_2, \frac{\Pi}{2}} \\
C'_2 &\DefineAs \truncate\of{\gk_2, \frac{\Pi}{2} - C_2} \\
\gd C_2 &\DefineAs \round{\frac{\Pi}{2} - C_2 - C'_2}
\end{dcases}
\end{equation*}
Equation (\ref{eqntruncerror}) applied to the definition of $C_2$ yields:
\begin{equation}
2^{\gk'_2 - 1} \ULP\of{\frac{\Pi}{2}} \leq \frac{\Pi}{2} - C_2 < 2^{\gk'_2} \ULP\of{\frac{\Pi}{2}}
\label{eqnc2}
\end{equation}
where $\gk'_2 \leq \gk_2$ accounts for any leading zeroes in the bits of $\frac{\Pi}{2}$ that are being truncated.  Accordingly equation (\ref{eqnulptr}) yields, for the unit of the last place:
\[
\ULP\of{\frac{\Pi}{2} - C_2} = 2^{\gk'_2 - M} \ULP\of{\frac{\Pi}{2}}
\]

Similarly, equation (\ref{eqntruncerror}) applied to the definition of $C'_2$ yields:
\begin{alignat*}{2}
2^{\gk''_2 - 1} \ULP\of{\frac{\Pi}{2} - C_2} &\leq \frac{\Pi}{2} - C_2 - C'_2 &< 2^{\gk''_2} \ULP\of{\frac{\Pi}{2} - C_2} \\
2^{\gk'_2 + \gk''_2 - M - 1} \ULP\of{\frac{\Pi}{2}} &\leq &< 2^{\gk'_2 + \gk''_2 - M} \ULP\of{\frac{\Pi}{2}}
\end{alignat*}
where $\gk''_2 \leq \gk_2$ accounts for any leading zeroes in the bits of $\frac{\Pi}{2} - C_2$ that are being truncated.  Note that normalization of the significand of $\frac{\Pi}{2} - C_2$ effectively drops the zeroes at positions $\gk_2$ to $\gk'_2$ and therefore the computation of $C'_2$ applies to a significand aligned on position $\gk'_2$.

It is straightforward to transform these inequalities using (\ref{eqnc2}) to obtain bounds on $C'_2$:
\[
2^{\gk'_2} \pa{\frac{1}{2} - 2^{\gk''_2 - M}} \ULP\of{\frac{\Pi}{2}} < C'_2 < 2^{\gk'_2} \pa{1 - 2^{\gk''_2 - M - 1}} \ULP\of{\frac{\Pi}{2}}
\]

Equation (\ref{eqnulptr}) applied to the definition of $C'_2$ yields, for the unit of the last place:
\begin{align*}
\ULP\of{\frac{\Pi}{2} - C_2 - C'_2} &= 2^{\gk''_2 - M} \ULP\of{\frac{\Pi}{2} - C_2} \\
&= 2^{\gk'_2 + \gk''_2 - 2 M} \ULP\of{\frac{\Pi}{2}}
\end{align*}

Noting that the absolute error on the rounding that appears in the definition of $\gd C_2$ is bounded by $\frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_2 - C'_2}$, we obtain the absolute error on the three-term approximation:
\begin{align}
\abs{\frac{\Pi}{2} - C_2 - C'_2 - \gd C_2} \leq \frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_2 - C'_2} = 2^{\gk'_2 + \gk''_2 - 2 M - 1} \ULP\of{\frac{\Pi}{2}}
\label{eqnpithreeterms}
\end{align}
and the following upper bound for $\gd C_2$:
\begin{equation}
\abs{\gd C_2} < 2^{\gk'_2 + \gk''_2 - M} \pa{1 + 2^{-M - 1}} \ULP\of{\frac{\Pi}{2}}
\label{eqnabsdc2}
\end{equation}
 
This scheme gives a representation with a significand that has effectively $3 M - \gk'_2 - \gk''_2$ bits and is such that multiplying $C_2$ and $C'_2$ by an integer less than or equal to $2^{\gk_2}$ is exact.

\section*{Argument Reduction}
Given an argument $x$, the purpose of argument reduction is to compute a pair of floating-point numbers $\pa{\red x, \gd \red x}$ such that:
\[
\begin{dcases}
\red x + \gd \red x ≅ x \pmod{\frac{\Pi}{2}} \\
\red x \;\text{is approximately in}\; \intclos{-\frac{\Pi}{4}}{\frac{\Pi}{4}} \\
\abs{\gd \red x} \leq \frac{1}{2} \ULP\of{\red x} 
\end{dcases}
\]

\subsection*{Argument Reduction for Small Angles}
If $\abs x < \round{\frac{\Pi}{4}}$ then $\red x = x$ and $\gd \red x = 0$.

\subsection*{Argument Reduction Using the Two-Term Approximation}\marginnote{TODO(phl): In this section and the next, document the ``better'' bounds computed with \textit{Mathematica}.}
If $\abs x \leq 2^{\gk_1} \round{\frac{\Pi}{2}}$ we compute:
\[
\begin{dcases}
n &= \iround{\round{x \round{\frac{2}{\Pi}}}} \\
y &= x - n \; C_1 \\
\gd y &= \round{n \; \gd C_1} \\
\pa{\red x, \gd \red x} &= \twodifference\of{y, \gd y}
\end{dcases}
\]
The first thing to note is that $\abs n \leq 2^{\gk_1}$.  We have:
\[
\abs x \leq 2^{\gk_1} \round{\frac{\Pi}{2}} = 2^{\gk_1} \frac{\Pi}{2} \pa{1 + \gd_1} 
\]
and:
\begin{equation}
\round{x \round{\frac{2}{\Pi}}} = x \frac{2}{\Pi} \pa{1 + \gd_2}\pa{1 + \gd_3}
\label{eqnxerror}
\end{equation}
from which we deduce the upper bound:
\begin{align*}
\abs n &\leq \iround{2^{\gk_1} \frac{\Pi}{2} \pa{1 + \gd_1} \frac{2}{\Pi} \pa{1 + \gd_2} \pa{1 + \gd_3}} \\
&\leq \iround{2^{\gk_1} \pa{1 + \gg_3}}
\end{align*}
If $2^{\gk_1} \gg_3$ is small enough (less than $1/2$), the rounding cannot cause $n$ to exceed $2^{\gk_1}$.  In practice we choose a relatively small value for $\gk_1$, so this condition is met.

Now if $x$ is close to an odd multiple of $\frac{\Pi}{4}$ it is possible for misrounding to happen.   There are two kinds of misrounding, with different bounds.

A misrounding of the first kind occurs if, assuming $n > 0$:
\[
x < \pa{n - \frac{1}{2}}\frac{\Pi}{2} \quad \text{and} \quad \round{x \round{\frac{2}{\Pi}}} > n - \frac{1}{2}
\]
Using equation (\ref{eqnxerror}) we find that this misrounding is only possible if:
\[
x > \frac{\Pi}{2} \pa{n - \frac{1}{2}} \frac{1}{\pa{1 + \gd_2} \pa{1 + \gd_3}} \geq \frac{\Pi}{2} \pa{n - \frac{1}{2}} \frac{1}{1 + \gg_2}
\]
In which case the computation of $n$ results in:
\[
n \frac{\Pi}{2} - x < \frac{\Pi}{4} \pa{1 + \frac{\gg_2}{1 + \gg_2} \pa{2 n - 1}}
\]
In this case, misrounding causes the absolute value of the reduced angle to increase and it may thus exceed $\frac{\Pi}{4}$ by as much as:
\begin{equation}
\frac{\Pi}{4} \frac{\gg_2}{1 + \gg_2} \pa{2^{\gk_1 + 1} - 1}
\label{eqnmisroundingbound}
\end{equation}
The accurate tables must be constructed so that the last interval covers angles misrounded in that manner\footnote{In practice this is not a stringent constraint because the distance between accurate table entries is much larger than the quantity given by (\ref{eqnmisroundingbound}).}.

A misrounding of the second kind occurs if, assuming $n \geq 0$:
\[
x > \pa{n + \frac{1}{2}}\frac{\Pi}{2} \quad \text{and} \quad \round{x \round{\frac{2}{\Pi}}} < n + \frac{1}{2}
\]
A derivation similar to the one above gives the following condition for this misrounding to be possible.  Using equation (\ref{eqnxerror}):
\[
x < \frac{\Pi}{2} \pa{n + \frac{1}{2}} \frac{1}{\pa{1 + \gd_2} \pa{1 + \gd_3}} \leq \frac{\Pi}{2} \pa{n + \frac{1}{2}} \pa{1 + \gg_2}
\]
we derive the bound:
\[
x - n \frac{\Pi}{2} < \frac{\Pi}{4} \pa{1 + \gg_2 \pa{2 n + 1}}
\]
In this case, misrounding causes the absolute value of the reduced angle to decrease by as much as:
\[
\frac{\Pi}{4} \gg_2 \pa{2^{\gk_1 + 1} + 1}
\]
This is however not a concern for the accurate tables and the polynomials as it cannot cause the reduced angle to become negative.

Using the bound on $\abs n$ and the fact that $C_1$ has $\gk_1$ trailing zeroes, we see that the product $n \; C_1$ is exact.  The subtraction $x - n \; C_1$ is exact by Sterbenz's Lemma.  Finally, the last step performs an exact addition\footnote{The more efficient $\quicktwodifference$ is not usable here.  First, note that $\abs y$ is equal to $\ULP\of{x}$ if we take $x$ to be the successor or the predecessor of $n C_1$ for any $n$. Ignoring rounding errors we have:
\[ 
\abs{\gd y} \geq n \; 2^{\gk'_1 - 1} \ULP\of{\frac{\Pi}{2}} \geq 2^{\gk'_1 + M - 2} \ULP\of{\frac{\Pi}{2}} \ULP\of{n}
\]
where we used the bound given by equation (\ref{eqnulp}).  Now the computation of $n$ can result in a value that is either in the same binade or in the binade below that of $x$.  Therefore $\ULP\of{n} \geq \frac{1}{2} \ULP\of{x}$ and the above inequality becomes:
\[
\abs{\gd y} \geq 2^{\gk'_1 + M - 3} \ULP\of{\frac{\Pi}{2}} \ULP\of{x}
\]
plugging $\ULP\of{\frac{\Pi}{2}} = 2^{1 - M}$ we find:
\[
\abs{\gd y} \geq 2^{\gk'_1 - 2} \ULP\of{x}
\]
Therefore, as long as $\gk'_1 > 2$, there exist arguments $x$ for which $\abs{\gd y} > \abs y$.
} using algorithm 4 of \cite{HidaLiBailey2007}.

To compute the overall error on argument reduction\footnote{Note that this error analysis is correct even in the face of misrounding.}, first remember that, from equation (\ref{eqnpitwoterms}), we have:
\[
C_1 + \gd C_1 = \frac{\Pi}{2} + \gz \quad \text{with} \quad \abs{\gz} \leq 2^{\gk'_1 - M - 1} \ULP\of{\frac{\Pi}{2}}
\]
The error computation proceeds as follows:
\begin{align*}
y - \gd y &= x - n \; C_1 - n \; \gd C_1 \pa{1 + \gd_4} \\
&= x - n \pa{C_1 + \gd C_1} - n \; \gd C_1 \; \gd_4 \\
&= x - n \frac{\Pi}{2} - n \pa{\gz + \gd C_1 \; \gd_4}
\end{align*}
from which we deduce an upper bound on the absolute error of the reduction:
\begin{align*}
\abs{y - \gd y - \pa{x - n \frac{\Pi}{2}}} &\leq 2^{\gk_1} 2^{\gk'_1} \pa{2^{- M - 1} + 2^{-M} + 2^{-2 M - 1}} \ULP\of{\frac{\Pi}{2}} \\
&= 2^{\gk_1 + \gk'_1 - M}\pa{\frac{3}{2} + 2^{-M - 1}} \ULP\of{\frac{\Pi}{2}} \\
&< 2^{\gk_1 + \gk'_1 - M + 1} \ULP\of{\frac{\Pi}{2}}
\end{align*}
where we have used the upper bound for $\gd C_1$ given by equation (\ref{eqnabsdc1}).

The exact $\twodifference$ yields a pair such that $\abs{\gd \red x} \le \frac{\ULP\of{\red x}}{2}$ which implies the bounds:
\begin{align*}
\text{Relative:} \qquad &\abs{\gd \red x} \le \frac{\abs{\red x}}{2^M} \\
\text{Absolute:} \qquad &\abs{\gd \red x} \le \frac{\ULP\of{\frac{\Pi}{4}}}{2} = 2^{-M - 1}
\end{align*}

Furthermore, misrounding of the first kind and the above error on the reduction may combine to cause $\abs{\red x}$ to move above $\frac{\pi}{4}$ by as much as:
\[
\frac{\Pi}{4} \frac{\gg_2}{1 + \gg_2} \pa{2^{\gk_1 + 1} - 1} + 2^{\gk_1 + \gk'_1 - M + 1} \ULP\of{\frac{\Pi}{2}}
\]
This must be taken into account when building the polynomials.

In the computation of the trigonometric functions, we need $\red x + \gd \red x$ to provide enough accuracy that the final result is correctly rounded most of the time.  The above error bound shows that, if $\red x$ is very small (i.e., if $x$ is very close to a multiple of $\frac{\Pi}{2}$), the two-term approximation may not provide enough correct bits.  Formally, say that we want to have $M + \gk_3$ correct bits in the mantissa of $\red x + \gd \red x$.  The error must be less than $2^{-\gk_3}$ half-units of the last place of the result:
\begin{equation}
2^{\gk_1 + \gk'_1 - M + 1} \ULP\of{\frac{\Pi}{2}} \leq 2^{-\gk_3 - 1} \ULP\of{\red x} \leq 2^{-\gk_3 - M} \abs{\red x}
\label{eqnreductionbound}
\end{equation}
which leads to the following condition on the reduced angle:
\[
\abs{\red x} \geq 2^{\gk_1 + \gk'_1 + \gk_3 + 1} \ULP\of{\frac{\Pi}{2}} = 2^{\gk_1 + \gk'_1 + \gk_3 - M + 2}
\]

The rest of the implementation assumes that $\gk_3 = 18$ to achieve correct rounding with high probability.  If we choose $\gk_1 = 8$ we find that $\gk'_1 = 5$ (because there are three consecutive zeroes at this location in the significand of $\frac{\Pi}{2}$) and the desired accuracy is obtained as long as $\abs{\red x} \geq 2^{-20} \simeq 9.5 \times 10^{-7}$.

\subsection*{Argument Reduction Using the Three-Term Approximation}
If $\abs x \leq 2^{\gk_2} \round{\frac{\Pi}{2}}$ we compute:
\[
\begin{dcases}
n &= \iround{\round{x \round{\frac{2}{\Pi}}}} \\
y &= x - n \; C_2 \\
y' &= n \; C'_2 \\
\gd y &= \round{n \; \gd C_2} \\
\pa{z, \gd z} &= \quicktwosum\of{y', \gd y} \\
\pa{\red x, \gd \red x} &= \longsub\of{y, \pa{z, \gd z}}
\end{dcases}
\]
The products $n \; C_2$ and $n \; C'_2$ are exact thanks to the $\gk_2$ trailing zeroes of $C_2$ and $C'_2$.  The subtraction $x - n \; C_2$ is exact by Sterbenz's Lemma.  $\quicktwosum$ performs an exact addition using algorithm 3 of \cite{HidaLiBailey2007}; it is usable in this case because clearly $\abs{\gd y} < \abs{y'}$.
$\longsub$ is the obvious adaptation of the algorithm $\longadd$ presented in section 5 of \cite{Linnainmaa1981}, which implements precise (but not exact) double-precision arithmetic.

It is straightforward to show, like we did in the preceding section, that:
\[
\abs n \leq \iround{2^{\gk_2} \pa{1 + \gg_3}}
\]
and therefore that $\abs n \leq 2^{\gk_2}$ as long as $2^{\gk_2} \gg_3 < 1/2$.  Similarly, the misrounding bound (\ref{eqnmisroundingbound}) is applicable with $\gk_2$ replacing $\gk_1$.

To compute the overall error on argument reduction, first remember that, from equation (\ref{eqnpithreeterms}), we have:
\[
C_2 + C'_2 + \gd C_2 = \frac{\Pi}{2} + \gz_1 \quad \text{with} \quad \abs{\gz_1} \leq 2^{\gk'_2 + \gk''_2 - 2 M - 1} \ULP\of{\frac{\Pi}{2}}
\]
Let $\gz_2$ be the relative error introduced by $\longadd$.  Table 1 of \cite{Linnainmaa1981} indicates that $\abs{\gz_2} < 2^{2 - 2 M}$.  The error computation proceeds as follows:
\begin{align*}
y - y' - \gd y &= \pa{x - n \; C_2 - n \; C'_2 - n \; \gd C_2 \pa{1 + \gd_4}} \pa{1 + \gz_2} \\
&= \pa{x - n \frac{\Pi}{2} - n \pa{\gz_1 + \gd C_2 \; \gd_4}} \pa{1 + \gz_2} \\
&= x - n \frac{\Pi}{2} - n \pa{\gz_1 + \gd C_2 \; \gd_4} \pa{1 + \gz_2} + \pa{x - n \frac{\Pi}{2}} \; \gz_2
\end{align*}
from which we deduce an upper bound on the absolute error of the reduction, noting that $\abs{x - n \frac{\Pi}{2}} \leq \frac{\Pi}{4}\pa{1 + \gg_2 \pa{2^{\gk_2 + 1} + 1}}$ as per (\ref{eqnmisroundingbound}):
\begin{align*}
&\abs{y - y' - \gd y - \pa{x - n \frac{\Pi}{2}}} \\ 
& \qquad \leq 2^{\gk_2 + \gk'_2 + \gk''_2} \pa{2^{-2 M - 1} + 2^{-2 M} + 2^{-3 M - 1}} \pa{1 + 2^{2 - 2 M}} \ULP\of{\frac{\Pi}{2}} + 2^{2 - 2 M} \frac{\Pi}{4}\pa{1 + \gg_2 \pa{2^{\gk_2 + 1} + 1}} \\
& \qquad < 2^{\gk_2 + \gk'_2 + \gk''_2 - 2 M} \pa{\frac{3}{2} + 2^{-M - 1}} \pa{1 + 2^{2 - 2 M}} \ULP\of{\frac{\Pi}{2}} + 2^{-2 M} \; \Pi \pa{1 + 3 \times 2^{\gk_2} \ULP\of{\frac{\Pi}{2}}} \\
& \qquad < 2^{\gk_2 - 2 M} \pa{2^{\gk'_2 + \gk''_2 + 1} + 3} \ULP\of{\frac{\Pi}{2}} + 2^{-2 M} \; \Pi
\end{align*}
where the second inequality uses $\gg_2 \pa{2^{\gk_2 + 1} + 1} < 3 u \; 2^{\gk_2 + 1}$.

A sufficient condition for the reduction to guarantee $\gk_3$ extra bits of accuracy is for this error to be less than $2^{-\gk_3 - 1} \ULP\of{\red x}$ which itself is less than $2^{-\gk_3 - M} \abs{\red{x}}$.  Therefore we want:
\begin{align*}
\abs{\red x} &\geq 2^{\gk_3 - M} \pa{2^{\gk_2} \pa{2^{\gk'_2 + \gk''_2 + 1} + 3} \ULP\of{\frac{\Pi}{2}} + \Pi} \\
&= 2^{\gk_3 - M} \pa{2^{\gk_2 - M + 1} \pa{2^{\gk'_2 + \gk''_2 + 1} + 3} + \Pi}
\end{align*}
and it is therefore sufficient to have:
\[
\abs{\red x} \geq 2^{\gk_3 - M} \pa{2^{\gk_2 + \gk'_2 + \gk''_2 - M + 2} + 4}
\]

If we choose $\gk_3 = 18$ as above, and $\gk_2 = 18$ we find that $\gk'_2 = 14$ and $\gk''_2 = 15$.  Therefore, the desired accuracy is obtained as long as $\abs{\red x} \geq 65 \times 2^{-39} \simeq 1.2 \times 10^{-10}$.

\subsection*{Fallback}
If any of the conditions above is not met, we fall back on the CORE-MATH implementation.

\section*{Accurate Tables and Their Generation}
\marginnote{TODO(phl): Document the tables and all the tricks that went into their generation.}
\section*{Polynomial Approximations}
The \textit{Mathematica} function \texttt{GeneralMiniMaxApproximation} produces a minimax polynomial $p$ such that $p\of{q\of{x}}$ approximates a function $f\of{x}$ by minimizing the quantity $\frac{f\of{x} - p\of{q\of{x}}}{g\of{x}}$.  By choosing $g\of{x}$ appropriately, we can obtain an approximation that minimizes either the absolute or relative error on the result.
\subsection*{Sin Near Zero}
For the $\sin$ function near zero the accurate tables method is not usable because the correction term is not small compared to the tabulated value of the function (which would be zero)\footnote{It would be possible to have one set of tables per binade with progressively denser intervals, but that would have a terrible performance as the tables would end up being very large.}.  Instead we use a polynomial approximation that minimizes the relative error on the result.  Since $\sin x$ is an even function and since its dominant term is $x$, we are looking for an approximation having the form:
\[
\sin x \simeq x + x^3 p_{s0}\of{x^2}
\]
over the interval $\intclos{0}{\gD}$, where $\gD$ is chosen so that $\gD^2 \ll 1$.

We are therefore calling \texttt{GeneralMiniMaxApproximation} with:
\[
\begin{dcases}
q\of{x} &\DefineAs x^2 \\
f\of{x} &\DefineAs \frac{\sin x - x}{x^3} \\
g\of{x} &\DefineAs \frac{\sin x}{x^3}
\end{dcases}
\]
which results in a polynomial $p_{s0}$ which minimizes the relative error on $\sin x$; the degree of $p_{s0}$ is choosen so that the error is less than $\ULP\of{\gD^2}$.

In practice we choose $\gD = 2^{-10}$, and compute a degree-1 polynomial which induces a relative error smaller than $2^{-75.538}$ (before rounding the coefficients to machine numbers).

\subsection*{Around Table Entries}

Let $\pa{x_k, s_k, c_k}$ be an accurate table entry.  $x_k$ is close to $2 k \gD$ and the accurate table interval\footnote{Obviously the intervals $\mathscr{I}_k$ cannot all contain their bounds.  Because of the way they are computed, the odd multiples of $\gD$ which separate the intervals are rounded to the nearest even $k$.  Therefore, there is an alternation of open-open intervals (for $k$ odd) and closed-closed intervals (for $k$ even).  We do not have a convenient notation for this, and anyway this detail is mostly irrelevant.} containing $x_k$ is $\mathscr{I}_k \DefineAs \intclos{\pa{2 k - 1}\gD}{\pa{2 k + 1}\gD}$ (for $k = 0$ the interval is $\mathscr{I}_0 \DefineAs \intclos{0}{\gD}$ and $x_0 = 0$).  The implementation of $\sin$ and $\cos$ starts by choosing, from the argument $x$ (which in the case of $\sin$, is not close to zero), a $k$ such that $x \in \mathscr{I}_k$ and computes $h = x - x_k$.  We are therefore looking for approximations of the form:
\begin{align*}
\sin h &\simeq h + h^3 p_s\of{h^2} \\
\cos h &\simeq 1 + h^2 p_c\of{h^2}
\end{align*}
which must cover the interval $\intclos{0}{h_{max}}$ with:
\[
h_{max} \DefineAs \max\limits_{k}\curlyBrackets{x_k - \pa{2 k - 1}\gD, \pa{2 k + 1}\gD - x_k}
\]

The error function used in these approximations is guided by the error analysis.  We will see below (see \nameref{secerroranalysissin} and \nameref{secerroranalysiscos}) that the analysis involves an expression like:
\begin{align*}
h \pa{2 \; \gd \red x + h} p_c\of{h^2} &= h \pa{2 \; \gd \red x + h} \pa{\frac{\cos h - 1}{h^2} + \mathscr{E}\of{h}} \\
&= \cos h - 1 + 2 \; \gd \red x \frac{\cos h - 1}{h} + 2 h \; \gd \red x \; \mathscr{E}\of{h}
\end{align*}
where $\mathscr{E}\of{h}$ is an error term resulting from the minimax approximation.  For this expression to behave properly when $h \to 0$ it is necessary that $\mathscr{E}\of{h} = \BigO\of{1}$.  This is not a trivial property: it is not verified by approximations with a relative error or an absolute error on $\cos$.  While this leaves us with many choices for the error function, the simplest one is $ \mathscr{E}\of{h} \propto \pa{\cos h - 1}/h^2$.  Therefore, for $\sin$ we call \texttt{GeneralMiniMaxApproximation} with:
\[
\begin{dcases}
q\of{h} &\DefineAs h^2 \\
f\of{h} &\DefineAs \frac{\sin h - h}{h^3} \\
g\of{h} &\DefineAs \frac{\sin h - h}{h^3}
\end{dcases}
\]
and for $\cos$:
\[
\begin{dcases}
q\of{h} &\DefineAs h^2 \\
f\of{h} &\DefineAs \frac{\cos h - 1}{h^2} \\
g\of{h} &\DefineAs \frac{\cos h - 1}{h^2}
\end{dcases}
\]

In practice we chose above $\gD = 2^{-10}$\marginnote{TODO(phl): Cross-reference the accurate tables section.} and the accurate tables construction yielded $h_{max} < \gD + 2^{-17.834}$.  The minimax computation results in a 1-degree polynomial $p_s$ with an error smaller than $2^{-52.689}$ and a 1-degree polynomial $p_c$ with an error smaller than $2^{-51.466}$ (before rounding the coefficients to machine numbers).

\section*{Core Implementation}

This section documents the core of the algorithms used to compute $\sin$ and $\cos$.  They take as input the result of argument reduction, $\pa{\red x, \gd \red x}$ and produce a pair $\pa{y, \gd y}$ which is passed to the rounding test described in \cite[397]{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010} to decide whether $\round{y + \gd y}$ is the correctly-rounded result (this is the case with high probability) or whether we need to fall back to the CORE-MATH implementation\marginnote{TODO(phl): Give the values of $e$.}.

We use $\roundAll{expr}$ to denote evaluation where appropriate rounding happens on each literal or operation of the expression $expr$.  This notation is typically used for error intervals computed using the functions \texttt{IEEEEvaluateWithRelativeError} and \texttt{IEEEEvaluateWithAbsoluteError} in file \href{https://github.com/mockingbirdnest/Principia/blob/master/mathematica/ieee754_floating_point_evaluation.wl}{\texttt{mathematica/ieee754\_floating\_point\_{\linebreak}evaluation.wl}}.  In particular, these functions take into account that the evaluation of the polynomials $p_{s0}$, $p_s$, and $p_c$ has two sources of errors: the rounding of the coefficients to machine numbers, and the error due to the floating-point operations; this may leading to asymmetrical error intervals. \marginnote{TODO(phl): Try to tune the high-degree coefficient after rounding the low-degree one.}

We assume that $\red x$ is positive ($\gd \red x$ may be positive or negative), that an FMA operation is available, and that the rounding direction is $roundTiesToEven$.

\subsection*{Reduced Angle}

Argument reduction took the input angle $x$ and produced a pair $\pa{\red x, \gd \red x}$ approximating the angle reduced modulo $\frac{\Pi}{4}$.  That approximation is correct to $M + \gk_3$ bits.  We therefore have, from equation (\ref{eqnreductionbound}):
\[
x \equiv \red x + \gd \red x + \gz_0 \; \red x \pmod{\frac{\Pi}{4}} \qquad \abs{\gz_0} \le 2^{-\gk_3 - M}
\]
For simplicity of the error analysis we can assume $0 \le x \le \frac{\Pi}{4}$: even though we do not do argument reduction when $x$ is less than $\frac{\Pi}{4}$, the error analysis would work exactly the same if $x$ was simply provided as a high-accuracy value (e.g., a double-double) and reduced using the techniques above.

\subsection*{Sin Near Zero}\label{secsinnearzero}
If $\abs{\red x} \le \gD = 2^{-10}$ the steps of the computation are as follows:
\[
\begin{dcases}
t_1 &= \roundAll{p_{s0}\of{{\red x}^2}} \\
t_2 &= \roundAll{{\red x}^3} \\
t_3 &= \round{t_1 t_2 + \gd \red x} \\
y &= \red x \\
\gd y &= t_3
\end{dcases}
\]

First note that, in the computation, the terms ${\red x}^n \; \gd \red x$ for $n > 0$ are irrelevant\marginnote{TODO(phl): This needs a more rigorous analysis, same for the other sections.}.  The reason is that $\abs{\red x \; \gd \red x} < \gD^2 / 2^M = 2^{-M - 20}$.  But we chose $\gk_3 = 18$ so we are already willing to tolerate errors significantly larger than $\red x \; \gd \red x$.  This is why we do not involve $\gd \red x$ in steps $t_1$ and $t_2$ above: specifically the most significant term in $t_1 t_2$ would be $\frac{1}{2}{\red x}^2 \; \gd \red x$ which corresponds to a relative error of $2^{-73.999}$, much smaller to the one we get from the analysis below.

The relative error on the minimax polynomial may be rewritten as:
\[
p_{s0}\of{x^2} = \frac{\pa{1 + \gz_1} \sin x - x}{x^3} \qquad \abs{\gz_1} < 2^{-75.538}
\]
for $\abs x \le \gD$.

The errors committed at each step are as follows:
\[
\left\{
\begin{alignedat}{3}
t_1 &= p_{s0}\of{{\red x}^2} \pa{1 + \gz_2} &\gz_2 &\in \intopen{-2^{-52.415}}{2^{-53.999}} \\
&= \frac{\pa{1 + \gz_1} \sin {\red x} - x}{{\red x^3}} \pa{1 + \gz_2} \\
t_2 &= {\red x}^3 \pa{1 + \gd_1} \pa{1 + \gd_2} \\
t_3 &= \pa{t_1 t_2 + \gd \red x} \pa{1 + \gd_3}
\end{alignedat}
\right.
\]
The relative error on the entire computation is:
\[
r\of{\red x, \gd \red x} \DefineAs \frac{y + \gd y}{\sin\of{\red x + \gd \red x + \gz_0 \; \red x}} - 1
\]
The function $r$ can be computed using \textit{Mathematica} interval arithmetic over the triangular domain $\abs{\red x} \le \gD$, $\abs{\gd \red x} \le 2^{-M} \abs{\red x}$.  Plotting shows that it is monotonic and its extrema are therefore reached at the corners of the domain.  In practice we find that $\abs{r\of{\red x, \gd \red x}} < 2^{-70.561}$ (as expected, this is a bit worse than the error originating from the angle reduction).\marginnote{TODO(phl): $\gk_3 = 19$ would gain 0.7 bits.  Worthwhile?}

\subsection*{Around Table Entries}
If $\red x \in \mathscr{I}_k$ we first compute $h = \red x - x_k$.  It is essential that this subtraction be exact, and therefore that the conditions of Sterbenz's lemma be met: we must have $\red x / 2 \le x_k \le 2 \red x$.  Based on the range of $\red x$, a sufficient condition for this is:
\[
\frac{2 k + 1}{2} \gD \le x_k \le 2 \pa{2 k - 1} \gD
\]
Because $x_k$ is close to $2 k \gD$ the left part of this condition is trivially met for all $k > 0$.  However the second part is only trivially met if $k > 1$.  If $k = 1$ it becomes $x_k \le 2 \; \gD$.  In other words, when building the accurate tables we must only look for $x_1$ \emph{below} $2 \; \gD$.\marginnote{TODO(phl): Cross-reference in the accurate tables section.}

For $\sin$ we do not use the interval $\mathscr{I}_0$ (see \nameref{secsinnearzero}, above) but for $\cos$ we do.  That interval is special because $x_0 = 0$: the computation of $h$ is therefore trivially correct.

The relative errors on the minimax polynomials may be rewritten as:
\begin{align*}
p_s\of{h^2} &= \frac{\sin h - h}{h^3} \pa{1 + \gz_1} \qquad & \abs{\gz_1} < 2^{-52.689} \\
p_c\of{h^2} &= \frac{\cos h - 1}{h^2} \pa{1 + \gz_2} \qquad & \abs{\gz_2} < 2^{-51.466}
\end{align*}

\subsubsection*{Sin}\label{secerroranalysissin}

The first step of the computation is to evaluate $h c_k + s_k$ exactly using an FMA as explained in section 2.1 of \cite{StehléZimmermann2005}:
\[
\pa{z, \gd z} = t_0 = h c_k + s_k
\]
where $z$ and $\gd z$ have nonoverlapping significands.  For the purpose of describing the computation and analysing errors we will write $\gd z = t_0 \; \gd_0$ and $z = t_0 \pa{1 - \gd_0}$, where the two terms are exact.

The remaining steps of the computation are then as follows:
\[
\begin{dcases}
t_1 &= \roundAll{p_s\of{h^2}} \\
t_2 &= \roundAll{p_c\of{h^2}} \\
t_3 &= \round{h \round{2 \; \gd \red x + h}} \\
t_4 &= \roundAll{h^3} \\
t_5 &= \roundAll{\round{s_k} \; t3 \; t2} \\
t_6 &= \round{t4 \; t1} \\
t_7 &= \round{\round{c_k} t6 + t5} \\
t_8 &= \round{\round{c_k} \gd \red x + t_0 \; \gd_0} \\
t_9 &= \round{t7 + t8} \\
y &= t_0 \pa{1 - \gd_0} \\
\gd y &= t_9
\end{dcases}
\]
where we have made the rounding of the accurate table elements $s_k$ and $c_k$ explicit.

The errors commited at each step are as follows:
\[
\left\{
\begin{alignedat}{2}
t_1 &= p_s\of{h^2} \pa{1 + \gz_3} \qquad & \gz_3 \in \intopen{-2^{-54.946}}{2^{-52.200}} \\
&= \frac{\sin h - h}{h^3} \pa{1 + \gz_1} \pa{1 + \gz_3} \\
t_2 &= p_c\of{h^2} \pa{1 + \gz_4} \qquad & \gz_4 \in \intopen{-2^{-52.855}}{2^{-53.160}} \\
&= \frac{\cos h - 1}{h^2} \pa{1 + \gz_2} \pa{1 + \gz_4} \\
t_3 &= h \pa{2 \; \gd \red x + h} \pa{1 + \gd_1} \pa{1 + \gd_2} \\
t_4 &= h^3 \pa{1 + \gd_3} \pa{1 + \gd_4} \\
t_5 &= \round{s_k} \; t_3 \; t_2 \pa{1 + \gd_5} \pa{1 + \gd_6} \\
t_6 &= t_4 \; t_1 \pa{1 + \gd_7} \\
t_7 &= \pa{\round{c_k} \; t_6 + t_5} \pa{1 + \gd_8} \\
t_8 &= \pa{\round{c_k} \; \gd \red x + t_0 \; \gd_0} \pa{1 + \gd_9} \\
t_9 &= \pa{t_7 + t_8} \pa{1 + \gd_{10}}
\end{alignedat}
\right.
\]
The relative error on the entire computation is:
\[
r\of{\red x, \gd \red x} \DefineAs \frac{y + \gd y}{\sin\of{\red x + \gd \red x + \gz_0 \; \red x}} - 1
\]

Great care is required when evaluating this expression using interval arithmetic because the terms in $\gd_0$ in $t_9$ and $y$ could be considered independent and lead to a useless bound.  To avoid this issue $\gd_0$ must be factored out to appear only once.  Specifically the $\gd_0$ term after factoring is:
\[
\gd_0 \pa{h \round{c_k} + \round{s_k}}\pa{\pa{1 + \gd_{10}} \pa{1 + \gd_9} - 1}
\]
which is pleasantly small, and even though $\gd_9$ and $\gd_{10}$ also appear in the term that does not involve $\gd_0$, the effect of replicating them is acceptable.

The function $r$ must be evaluated separately for each interval around $x_k$, with $\red x \in \intclos{\pa{2 k - 1}\gD}{\pa{2 k + 1}\gD}$ and $\abs{\gd \red x} < {\abs{\red x}} / 2^M$.  This defines a trapezoidal domain and $r$ reaches its extrema at the corners of that domain.  In practice we find using \textit{Mathematica} interval arithmetic that $\abs{r\of{\red x, \gd \red x}} < 2^{-68.751}$.  The relative error for $k = 1$ is particularly high and is an outlier\marginnote{TODO(phl): Try to understand why.}.  Other than than, the worst relative errors correspond to table entries that have many ones after their accurate zeroes (or many zeroes after their accurate ones)\marginnote{TODO(phl): Should we try accurate tables with 19 bits?}.

\subsubsection*{Cos}\label{secerroranalysiscos}

The first step of the computation is to evaluate $-h s_k + c_k$ exactly using an FMA as explained in section 2.1 of \cite{StehléZimmermann2005}:
\[
\pa{z, \gd z} = t_0 = -h s_k + c_k
\]
where $z$ and $\gd z$ have nonoverlapping significands.  For the purpose of describing the computation and analysing errors we will write $\gd z = t_0 \; \gd_0$ and $z = t_0 \pa{1 - \gd_0}$, where the two terms are exact.

The remaining steps of the computation are then as follows:
\[
\begin{dcases}
t_1 &= \roundAll{p_s\of{h^2}} \\
t_2 &= \roundAll{p_c\of{h^2}} \\
t_3 &= \round{h \round{2 \; \gd \red x + h}} \\
t_4 &= \roundAll{h^3} \\
t_5 &= \roundAll{\round{c_k} \; t3 \; t2} \\
t_6 &= \round{t4 \; t1} \\
t_7 &= \round{-\round{s_k} t6 + t5} \\
t_8 &= \round{-\round{s_k} \gd \red x + t_0 \; \gd_0} \\
t_9 &= \round{t7 + t8} \\
y &= t_0 \pa{1 - \gd_0} \\
\gd y &= t_9
\end{dcases}
\]
where we have made the rounding of the accurate table elements $s_k$ and $c_k$ explicit.

The errors commited at each step are as follows:
\[
\left\{
\begin{alignedat}{2}
t_1 &= p_s\of{h^2} \pa{1 + \gz_3} \qquad & \gz_3 \in \intopen{-2^{-54.946}}{2^{-52.200}} \\
&= \frac{\sin h - h}{h^3} \pa{1 + \gz_1} \pa{1 + \gz_3} \\
t_2 &= p_c\of{h^2} \pa{1 + \gz_4} \qquad & \gz_4 \in \intopen{-2^{-52.855}}{2^{-53.160}} \\
&= \frac{\cos h - 1}{h^2} \pa{1 + \gz_2} \pa{1 + \gz_4} \\
t_3 &= h \pa{2 \; \gd \red x + h} \pa{1 + \gd_1} \pa{1 + \gd_2} \\
t_4 &= h^3 \pa{1 + \gd_3} \pa{1 + \gd_4} \\
t_5 &= \round{c_k} \; t_3 \; t_2 \pa{1 + \gd_5} \pa{1 + \gd_6} \\
t_6 &= t_4 \; t_1 \pa{1 + \gd_7} \\
t_7 &= \pa{\round{s_k} \; t_6 + t_5} \pa{1 + \gd_8} \\
t_8 &= \pa{\round{s_k} \; \gd \red x + t_0 \; \gd_0} \pa{1 + \gd_9} \\
t_9 &= \pa{t_7 + t_8} \pa{1 + \gd_{10}}
\end{alignedat}
\right.
\]
The relative error on the entire computation is:
\[
r\of{\red x, \gd \red x} \DefineAs \frac{y + \gd y}{\sin\of{\red x + \gd \red x + \gz_0 \; \red x}} - 1
\]
As explained above (see \nameref{secerroranalysissin}) the expression for $r$ must be rewritten so that $\gd_0$ only appears once.  In this case the $\gd_0$ term is:
\[
\gd_0 \pa{-\round{c_k} + h \round{s_k}} \pa{1 - \pa{1 + \gd_{10}} \pa{1 + \gd_9}}
\]
and a \textit{Mathematica} computation yields $\abs{r\of{\red x, \gd \red x}} < 2^{-69.217}$.  Again, the largest errors come from table entries that have many ones after their accurate zeroes (or many zeroes after their accurate ones).

\end{document}
