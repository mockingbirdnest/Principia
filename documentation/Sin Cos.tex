\documentclass[10pt, a4paper, twoside]{basestyle}

\usepackage{tikz}
\usetikzlibrary{cd}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage[Mathematics]{semtex}
\usepackage{chngcntr}
\counterwithout{equation}{section}

% Must come last.
\usepackage{siunitx}
\sisetup{math-micro=\text{µ},text-micro=µ}

%%%% Shorthands.
\DeclareMathOperator{\bias}{\mathit{bias}}
\DeclareMathOperator{\ULP}{\mathfrak u}
\DeclareMathOperator{\mant}{\mathfrak m}
\DeclareMathOperator{\expn}{\mathfrak e}
\DeclareMathOperator{\truncate}{\StandardSymbol{Tr}}
\DeclareMathOperator{\twosum}{\StandardSymbol{TwoSum}}
\DeclareMathOperator{\quicktwosum}{\StandardSymbol{QuickTwoSum}}
\DeclareMathOperator{\longadd}{\StandardSymbol{LongAdd}}
\DeclareMathOperator{\twodifference}{\StandardSymbol{TwoDifference}}
\DeclareMathOperator{\quicktwodifference}{\StandardSymbol{QuickTwoDifference}}
\DeclareMathOperator{\longsub}{\StandardSymbol{LongSub}}
\DeclareMathOperator{\cmod}{\StandardSymbol{cmod}}

% Rounding brackets will be heavily nested, and reading the nesting depth is critically important,
% so we make them grow for readability.
\newcommand{\round}[1]{\doubleSquareBrackets*{#1}}
\newcommand{\roundTowardZero}[1]{\doubleSquareBrackets{#1}_0}
\newcommand{\roundTowardPositive}[1]{\doubleSquareBrackets{#1}_+}
\newcommand{\roundTowardNegative}[1]{\doubleSquareBrackets{#1}_-}
\newcommand{\roundAll}[1]{\doubleSquareBrackets{#1}_⋯}
\newcommand{\hex}[1]{{_{16}}\mathrm{#1}}
\newcommand{\bin}[1]{{_{2}}\mathrm{#1}}
\newcommand{\red}[1]{\tilde{#1}}

%%%% Title and authors.

\title{An Implementation of Sin and Cos Using Gal's Accurate Tables}
\date{\printdate{2025-09-14}}
\author{Pascal~Leroy (phl)}
\begin{document}
\maketitle
\begin{sloppypar}
\noindent
This document describes the implementation of functions \texttt{Sin} and \texttt{Cos} in Principia.  The goals of that implementation are to be portable (including to machines that do not have a fused multiply-add instruction), achieve good performance, and ensure correct rounding.
\end{sloppypar}

\section*{Overview}

The implementation follows the ideas described by \cite{GalBachelis1991} and uses accurate tables produced by the method presented in \cite{StehléZimmermann2005}.  It guarantees correct rounding with a high probability.  In circumstances where it cannot guarantee correct rounding, it falls back to the (slower but correct) implementation provided by the CORE-MATH project \cite{SibidanovZimmermannGlondu2022} \cite{ZimmermannSibidanovGlondu2024}.  More precisely, the algorithm proceeds through the following steps:
\begin{itemize}[nosep]
\item perform argument reduction using Cody and Waite's algorithm in double precision (see \cite[379]{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010});
\item if argument reduction loses too many bits (\idest, the argument is close to a multiple of $\frac{\Pi}{2}$), fall back to \texttt{cr\_sin} or \texttt{cr\_cos};
\item otherwise, uses accurate tables and polynomial approximations to compute \texttt{Sin} or \texttt{Cos} with extra accuracy;
\item if the result fails Muller's rounding test (\cite[397-400]{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010}), fall back to \texttt{cr\_sin} or \texttt{cr\_cos};
\item otherwise return the rounded result of the preceding computation.
\end{itemize}

\section*{Notation and Accuracy Model} 

In this document we assume a base-2 floating-point number system with $M$ significand bits\footnote{In \texttt{binary64}, $M = 53$.} similar to the IEEE formats.  We define a real  function $\mant$ and an integer function $\expn$ denoting the \emph{significand} and \emph{exponent} of a real number, respectively:
\[
x = ±\mant\of{x} \times 2^{\expn\of{x}} \qquad\text{with}\qquad 2^{M-1} \leq \mant\of{x} \leq 2^M - 1
\]
Note that this representation is unique.  Furthermore, if $x$ is a floating-point number, $\mant\of{x}$ is an integer.

The \emph{unit of the last place\footnote{Intuitively, this is the ULP ``above'' for powers of 2.}} of $x$ is defined as:
\[
\ULP\of{x} \DefineAs 2^{\expn\of{x}}
\]
In particular, $\ULP\of{1} = 2^{1 - M}$ and:
\begin{equation}
\frac{\abs x}{2^M} < \frac{\abs x}{2^M - 1} \leq \ULP\of{x} \leq \frac{\abs x}{2^{M - 1}}
\label{eqnulp}
\end{equation}

We ignore the exponent bias, overflow and underflow as they play no role in this discussion.

Finally, for error analysis we use the accuracy model of \cite{Higham2002}, equation (2.4): everywhere they appear, the quantities $\gd_i$ represent a roundoff factor such that $\abs{\gd_i} < u = \ULP\of{\frac{1}{2}} = 2^{-M}$ (see pages 37 and 38).  We also use $\gq_n$ and $\gg_n$ with the same meaning as in \cite{Higham2002}, lemma 3.1.

\section*{Approximation of $\frac{\Pi}{2}$}

To perform argument reduction, we need to build approximations of $\frac{\Pi}{2}$ with extra accuracy and analyse the circumstances under which they may be used and the errors that they entail on the reduced argument.

Let $z \geq 0$.  We start by defining the truncation function $\truncate\of{\gk, z}$ which clears the last $\gk$ bits of the significand of $z$:
\[
\truncate\of{\gk, z} \DefineAs \floor{2^{-\gk} \mant \of{z}} \; 2^{\gk} \ULP\of{z}
\]
We have:
\[
z - \truncate\of{\gk, z} = \pa{2^{-\gk} \mant \of{z} - \floor{2^{-\gk} \mant \of{z}}} \; 2^{\gk} \ULP\of{z}
\]
The definition of the floor function implies that the quantity in parentheses is in $\intclop 0 1$ and therefore:
\[
0 \leq z - \truncate\of{\gk, z} < 2^{\gk} \ULP\of{z}
\]
Furthermore if the bits that are being truncated start with exactly $k$ zeros we have the stricter inequality:
\begin{equation}
2^{\gk' - 1} \ULP\of{z} \leq z - \truncate\of{\gk, z} < 2^{\gk'} \ULP\of{z} \quad \text{with} \quad \gk' = \gk - k
\label{eqntruncerror}
\end{equation}
This leads to the following upper bound for the unit of the last place of the truncation error:
\[
\ULP\of{z - \truncate\of{\gk, z}} < 2^{\gk' - M + 1} \ULP\of{z}
\]
which can be made more precise by noting that the function $\ULP$ is always a power of $2$:
\begin{equation}
\ULP\of{z - \truncate\of{\gk, z}} = 2^{\gk' - M} \ULP\of{z}
\label{eqnulptr}
\end{equation}

\subsubsection*{Two-Term Approximation}

In this scheme we approximate $\frac{\Pi}{2}$ as the sum of two floating-point numbers:
\[
\frac{\Pi}{2} \simeq C_1 + \gd C_1
\]
which are defined as:
\begin{equation*}
\begin{dcases}
C_1 &\DefineAs \truncate\of{\gk_1, \frac{\Pi}{2}} \\
\gd C_1 &\DefineAs \round{\frac{\Pi}{2} - C_1}
\end{dcases}
\end{equation*}
Equation (\ref{eqntruncerror}) applied to the definition of $C_1$ yields:
\[
2^{\gk'_1 - 1} \ULP\of{\frac{\Pi}{2}} \leq \frac{\Pi}{2} - C_1 < 2^{\gk'_1} \ULP\of{\frac{\Pi}{2}}
\]
where $\gk'_1 \leq \gk_1$ accounts for any leading zeroes in the bits of $\frac{\Pi}{2}$ that are being truncated.  Accordingly equation (\ref{eqnulptr}) yields, for the unit of the last place:
\[
\ULP\of{\frac{\Pi}{2} - C_1} = 2^{\gk'_1 - M} \ULP\of{\frac{\Pi}{2}}
\]

Noting that the absolute error on the rounding that appears in the definition of $\gd C_1$ is bounded by $\frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_1}$, we obtain the absolute error on the two-term approximation\footnote{With the chosen value $\gk_1 = 8$, the two sides of this inequality turn out to be $2^{-103.217}$ and $2^{-101}$.  The difference comes from the fact that $\abs{\frac{\Pi}{2} - C_1 - \gd C_1}$ has three zeroes after the last bit of its mantissa, but the theoretical computation above assumes the worst, \idest, a run of ones.}:
\begin{align}
\abs{\frac{\Pi}{2} - C_1 - \gd C_1} \leq \frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_1} = 2^{\gk'_1 - M - 1} \ULP\of{\frac{\Pi}{2}}
\label{eqnpitwoterms}
\end{align}


From this we derive the following upper bound for $\gd C_1$:
\begin{align}
\abs{\gd C_1} &< \frac{\Pi}{2} - C_1 + \frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_1} \nonumber \\
&< 2^{\gk'_1} \ULP\of{\frac{\Pi}{2}}+2^{\gk'_1 - M - 1} \ULP\of{\frac{\Pi}{2}} = 2^{\gk'_1} \pa{1 + 2^{-M - 1}} \ULP\of{\frac{\Pi}{2}}
\label{eqnabsdc1}
\end{align}
 
This scheme gives a representation with a significand that has effectively $2 M - \gk'_1$ bits and is such that multiplying $C_1$ by an integer less than or equal to $2^{\gk_1}$ is exact.

\subsubsection*{Three-Term Approximation}

In this scheme we approximate $\frac{\Pi}{2}$ as the sum of three floating-point numbers:
\[
\frac{\Pi}{2} \simeq C_2 + C'_2 + \gd C_2
\]
which are defined as:
\begin{equation*}
\begin{dcases}
C_2 &\DefineAs \truncate\of{\gk_2, \frac{\Pi}{2}} \\
C'_2 &\DefineAs \truncate\of{\gk_2, \frac{\Pi}{2} - C_2} \\
\gd C_2 &\DefineAs \round{\frac{\Pi}{2} - C_2 - C'_2}
\end{dcases}
\end{equation*}
Equation (\ref{eqntruncerror}) applied to the definition of $C_2$ yields:
\begin{equation}
2^{\gk'_2 - 1} \ULP\of{\frac{\Pi}{2}} \leq \frac{\Pi}{2} - C_2 < 2^{\gk'_2} \ULP\of{\frac{\Pi}{2}}
\label{eqnc2}
\end{equation}
where $\gk'_2 \leq \gk_2$ accounts for any leading zeroes in the bits of $\frac{\Pi}{2}$ that are being truncated.  Accordingly equation (\ref{eqnulptr}) yields, for the unit of the last place:
\[
\ULP\of{\frac{\Pi}{2} - C_2} = 2^{\gk'_2 - M} \ULP\of{\frac{\Pi}{2}}
\]

Similarly, equation (\ref{eqntruncerror}) applied to the definition of $C'_2$ yields:
\begin{alignat*}{2}
2^{\gk''_2 - 1} \ULP\of{\frac{\Pi}{2} - C_2} &\leq \frac{\Pi}{2} - C_2 - C'_2 &< 2^{\gk''_2} \ULP\of{\frac{\Pi}{2} - C_2} \\
2^{\gk'_2 + \gk''_2 - M - 1} \ULP\of{\frac{\Pi}{2}} &\leq &< 2^{\gk'_2 + \gk''_2 - M} \ULP\of{\frac{\Pi}{2}}
\end{alignat*}
where $\gk''_2 \leq \gk_2$ accounts for any leading zeroes in the bits of $\frac{\Pi}{2} - C_2$ that are being truncated.  Note that normalization of the significand of $\frac{\Pi}{2} - C_2$ effectively drops the zeroes at positions $\gk_2$ to $\gk'_2$ and therefore the computation of $C'_2$ applies to a significand aligned on position $\gk'_2$.

It is straightforward to transform these inequalities using (\ref{eqnc2}) to obtain bounds on $C'_2$:
\[
2^{\gk'_2} \pa{\frac{1}{2} - 2^{\gk''_2 - M}} \ULP\of{\frac{\Pi}{2}} < C'_2 < 2^{\gk'_2} \pa{1 - 2^{\gk''_2 - M - 1}} \ULP\of{\frac{\Pi}{2}}
\]

Equation (\ref{eqnulptr}) applied to the definition of $C'_2$ yields, for the unit of the last place:
\begin{align*}
\ULP\of{\frac{\Pi}{2} - C_2 - C'_2} &= 2^{\gk''_2 - M} \ULP\of{\frac{\Pi}{2} - C_2} \\
&= 2^{\gk'_2 + \gk''_2 - 2 M} \ULP\of{\frac{\Pi}{2}}
\end{align*}

Noting that the absolute error on the rounding that appears in the definition of $\gd C_2$ is bounded by $\frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_2 - C'_2}$, we obtain the absolute error on the three-term approximation:
\begin{align}
\abs{\frac{\Pi}{2} - C_2 - C'_2 - \gd C_2} \leq \frac{1}{2} \ULP\of{\frac{\Pi}{2} - C_2 - C'_2} = 2^{\gk'_2 + \gk''_2 - 2 M - 1} \ULP\of{\frac{\Pi}{2}}
\label{eqnpithreeterms}
\end{align}
and the following upper bound for $\gd C_2$:
\begin{equation}
\abs{\gd C_2} < 2^{\gk'_2 + \gk''_2 - M} \pa{1 + 2^{-M - 1}} \ULP\of{\frac{\Pi}{2}}
\label{eqnabsdc2}
\end{equation}
 
This scheme gives a representation with a significand that has effectively $3 M - \gk'_2 - \gk''_2$ bits and is such that multiplying $C_2$ and $C'_2$ by an integer less than or equal to $2^{\gk_2}$ is exact.

\section*{Argument Reduction}

Given an argument $x$, the purpose of argument reduction is to compute a pair of floating-point numbers $\pa{\red x, \gd \red x}$ such that:
\[
\begin{dcases}
\red x + \gd \red x ≅ x \pmod{\frac{\Pi}{2}} \\
\red x \;\text{is approximately in}\; \intclos{-\frac{\Pi}{4}}{\frac{\Pi}{4}} \\
\abs{\gd \red x} \leq \frac{1}{2} \ULP\of{\red x} 
\end{dcases}
\]

\subsection*{Argument Reduction for Small Angles}

If $\abs x < \round{\frac{\Pi}{4}}$ then $\red x = x$ and $\gd \red x = 0$.

\subsection*{Argument Reduction Using the Two-Term Approximation}

If $\abs x \leq 2^{\gk_1} \round{\frac{\Pi}{2}}$ we compute:
\[
\begin{dcases}
n &= \iround{\round{x \round{\frac{2}{\Pi}}}} \\
y &= x - n \; C_1 \\
\gd y &= \round{n \; \gd C_1} \\
\pa{\red x, \gd \red x} &= \twodifference\of{y, \gd y}
\end{dcases}
\]
The first thing to note is that $\abs n \leq 2^{\gk_1}$.  We have:
\[
\abs x \leq 2^{\gk_1} \round{\frac{\Pi}{2}} = 2^{\gk_1} \frac{\Pi}{2} \pa{1 + \gd_1} 
\]
and:
\begin{equation}
\round{x \round{\frac{2}{\Pi}}} = x \frac{2}{\Pi} \pa{1 + \gd_2}\pa{1 + \gd_3}
\label{eqnxerror}
\end{equation}
from which we deduce the upper bound:
\begin{align*}
\abs n &\leq \iround{2^{\gk_1} \frac{\Pi}{2} \pa{1 + \gd_1} \frac{2}{\Pi} \pa{1 + \gd_2} \pa{1 + \gd_3}} \\
&\leq \iround{2^{\gk_1} \pa{1 + \gg_3}}
\end{align*}
If $2^{\gk_1} \gg_3$ is small enough (less than $1/2$), the rounding cannot cause $n$ to exceed $2^{\gk_1}$.  In practice we choose a relatively small value for $\gk_1$, so this condition is met.

Now if $x$ is close to an odd multiple of $\frac{\Pi}{4}$ it is possible for misrounding to happen.   There are two kinds of misrounding, with different bounds.

A misrounding of the first kind occurs if, assuming $n > 0$:
\[
x < \pa{n - \frac{1}{2}}\frac{\Pi}{2} \quad \text{and} \quad \round{x \round{\frac{2}{\Pi}}} > n - \frac{1}{2}
\]
Using equation (\ref{eqnxerror}) we find that this misrounding is only possible if:
\[
x > \frac{\Pi}{2} \pa{n - \frac{1}{2}} \frac{1}{\pa{1 + \gd_2} \pa{1 + \gd_3}} \geq \frac{\Pi}{2} \pa{n - \frac{1}{2}} \frac{1}{1 + \gg_2}
\]
In which case the computation of $n$ results in:
\[
n \frac{\Pi}{2} - x < \frac{\Pi}{4} \pa{1 + \frac{\gg_2}{1 + \gg_2} \pa{2 n - 1}}
\]
In this case, misrounding causes the absolute value of the reduced angle to increase and it may thus exceed $\frac{\Pi}{4}$ by as much as:
\begin{equation}
\frac{\Pi}{4} \frac{\gg_2}{1 + \gg_2} \pa{2^{\gk_1 + 1} - 1}
\label{eqnmisroundingbound}
\end{equation}

A misrounding of the second kind occurs if, assuming $n \geq 0$:
\[
x > \pa{n + \frac{1}{2}}\frac{\Pi}{2} \quad \text{and} \quad \round{x \round{\frac{2}{\Pi}}} < n + \frac{1}{2}
\]
A derivation similar to the one above gives the following condition for this misrounding to be possible.  Using equation (\ref{eqnxerror}):
\[
x < \frac{\Pi}{2} \pa{n + \frac{1}{2}} \frac{1}{\pa{1 + \gd_2} \pa{1 + \gd_3}} \leq \frac{\Pi}{2} \pa{n + \frac{1}{2}} \pa{1 + \gg_2}
\]
we derive the bound:
\[
x - n \frac{\Pi}{2} < \frac{\Pi}{4} \pa{1 + \gg_2 \pa{2 n + 1}}
\]
In this case, misrounding causes the absolute value of the reduced angle to decrease by as much as:
\[
\frac{\Pi}{4} \gg_2 \pa{2^{\gk_1 + 1} + 1}
\]
This is however not a concern for the accurate tables as it cannot cause the reduced angle to become negative.

Using the bound on $\abs n$ and the fact that $C_1$ has $\gk_1$ trailing zeroes, we see that the product $n \; C_1$ is exact.  The subtraction $x - n \; C_1$ is exact by Sterbenz's Lemma.  Finally, the last step performs an exact addition\footnote{The more efficient $\quicktwodifference$ is not usable here.  First, note that $\abs y$ is equal to $\ULP\of{x}$ if we take $x$ to be the successor or the predecessor of $n C_1$ for any $n$. Ignoring rounding errors we have:
\[ 
\abs{\gd y} \geq n \; 2^{\gk'_1 - 1} \ULP\of{\frac{\Pi}{2}} \geq 2^{\gk'_1 + M - 2} \ULP\of{\frac{\Pi}{2}} \ULP\of{n}
\]
where we used the bound given by equation (\ref{eqnulp}).  Now the computation of $n$ can result in a value that is either in the same binade or in the binade below that of $x$.  Therefore $\ULP\of{n} \geq \frac{1}{2} \ULP\of{x}$ and the above inequality becomes:
\[
\abs{\gd y} \geq 2^{\gk'_1 + M - 3} \ULP\of{\frac{\Pi}{2}} \ULP\of{x}
\]
plugging $\ULP\of{\frac{\Pi}{2}} = 2^{1 - M}$ we find:
\[
\abs{\gd y} \geq 2^{\gk'_1 - 2} \ULP\of{x}
\]
Therefore, as long as $\gk'_1 > 2$, there exist arguments $x$ for which $\abs{\gd y} > \abs y$.
} using algorithm 4 of \cite{HidaLiBailey2007}.

To compute the overall error on argument reduction\footnote{Note that this error analysis is correct even in the face of misrounding.}, first remember that, from equation (\ref{eqnpitwoterms}), we have:
\[
C_1 + \gd C_1 = \frac{\Pi}{2} + \gz \quad \text{with} \quad \abs{\gz} \leq 2^{\gk'_1 - M - 1} \ULP\of{\frac{\Pi}{2}}
\]
The error computation proceeds as follows:
\begin{align*}
y - \gd y &= x - n \; C_1 - n \; \gd C_1 \pa{1 + \gd_4} \\
&= x - n \pa{C_1 + \gd C_1} - n \; \gd C_1 \; \gd_4 \\
&= x - n \frac{\Pi}{2} - n \pa{\gz + \gd C_1 \; \gd_4}
\end{align*}
from which we deduce an upper bound on the absolute error of the reduction:
\begin{align*}
\abs{y - \gd y - \pa{x - n \frac{\Pi}{2}}} &\leq 2^{\gk_1} 2^{\gk'_1} \pa{2^{- M - 1} + 2^{-M} + 2^{-2 M - 1}} \ULP\of{\frac{\Pi}{2}} \\
&= 2^{\gk_1 + \gk'_1 - M}\pa{\frac{3}{2} + 2^{-M - 1}} \ULP\of{\frac{\Pi}{2}} \\
&< 2^{\gk_1 + \gk'_1 - M + 1} \ULP\of{\frac{\Pi}{2}}
\end{align*}
where we have used the upper bound for $\gd C_1$ given by equation (\ref{eqnabsdc1}).

The exact $\twodifference$ yields a pair such that $\abs{\gd \red x} \le \frac{\ULP\of{\red x}}{2} \le 2^{-M} \abs{\red x}$.  Furthermore, misrounding of the first kind and the above error on the reduction may combine to cause $\abs{\red x}$ to move above $\frac{\pi}{4}$ by as much as:
\[
\frac{\Pi}{4} \frac{\gg_2}{1 + \gg_2} \pa{2^{\gk_1 + 1} - 1} + 2^{\gk_1 + \gk'_1 - M + 1} \ULP\of{\frac{\Pi}{2}}
\]
The accurate tables must be constructed so that the last interval covers angles misrounded in that manner\footnote{In practice this is not a stringent constraint because the distance between accurate table entries is much larger than this quantity.}.

In the computation of the trigonometric functions, we need $\red x + \gd \red x$ to provide enough accuracy that the final result is correctly rounded most of the time.  The above error bound shows that, if $\red x$ is very small (\idest, if $x$ is very close to a multiple of $\frac{\Pi}{2}$), the two-term approximation may not provide enough correct bits.  Formally, say that we want to have $M + \gk_3$ correct bits in the mantissa of $\red x + \gd \red x$.  The error must be less than $2^{-\gk_3}$ half-units of the last place of the result:
\begin{equation}
2^{\gk_1 + \gk'_1 - M + 1} \ULP\of{\frac{\Pi}{2}} \leq 2^{-\gk_3 - 1} \ULP\of{\red x} \leq 2^{-\gk_3 - M} \abs{\red x}
\label{eqnreductionbound}
\end{equation}
which leads to the following condition on the reduced angle:
\[
\abs{\red x} \geq 2^{\gk_1 + \gk'_1 + \gk_3 + 1} \ULP\of{\frac{\Pi}{2}} = 2^{\gk_1 + \gk'_1 + \gk_3 - M + 2}
\]

The rest of the implementation assumes that $\gk_3 = 18$ to achieve correct rounding with high probability.  If we choose $\gk_1 = 8$ we find that $\gk'_1 = 5$ (because there are three consecutive zeroes at this location in the significand of $\frac{\Pi}{2}$) and the desired accuracy is obtained as long as $\abs{\red x} \geq 2^{-20} \simeq 9.5 \times 10^{-7}$.

\subsection*{Argument Reduction Using the Three-Term Approximation}

If $\abs x \leq 2^{\gk_2} \round{\frac{\Pi}{2}}$ we compute:
\[
\begin{dcases}
n &= \iround{\round{x \round{\frac{2}{\Pi}}}} \\
y &= x - n \; C_2 \\
y' &= n \; C'_2 \\
\gd y &= \round{n \; \gd C_2} \\
\pa{z, \gd z} &= \quicktwosum\of{y', \gd y} \\
\pa{\red x, \gd \red x} &= \longsub\of{y, \pa{z, \gd z}}
\end{dcases}
\]
The products $n \; C_2$ and $n \; C'_2$ are exact thanks to the $\gk_2$ trailing zeroes of $C_2$ and $C'_2$.  The subtraction $x - n \; C_2$ is exact by Sterbenz's Lemma.  $\quicktwosum$ performs an exact addition using algorithm 3 of \cite{HidaLiBailey2007}; it is usable in this case because clearly $\abs{\gd y} < \abs{y'}$.
$\longsub$ is the obvious adaptation of the algorithm $\longadd$ presented in section 5 of \cite{Linnainmaa1981}, which implements precise (but not exact) double-precision arithmetic.

It is straightforward to show, like we did in the preceding section, that:
\[
\abs n \leq \iround{2^{\gk_2} \pa{1 + \gg_3}}
\]
and therefore that $\abs n \leq 2^{\gk_2}$ as long as $2^{\gk_2} \gg_3 < 1/2$.  Similarly, the misrounding bound (\ref{eqnmisroundingbound}) is applicable with $\gk_2$ replacing $\gk_1$.

To compute the overall error on argument reduction, first remember that, from equation (\ref{eqnpithreeterms}), we have:
\[
C_2 + C'_2 + \gd C_2 = \frac{\Pi}{2} + \gz_1 \quad \text{with} \quad \abs{\gz_1} \leq 2^{\gk'_2 + \gk''_2 - 2 M - 1} \ULP\of{\frac{\Pi}{2}}
\]
Let $\gz_2$ be the relative error introduced by $\longadd$.  Table 1 of \cite{Linnainmaa1981} indicates that $\abs{\gz_2} < 2^{2 - 2 M}$.  The error computation proceeds as follows:
\begin{align*}
y - y' - \gd y &= \pa{x - n \; C_2 - n \; C'_2 - n \; \gd C_2 \pa{1 + \gd_4}} \pa{1 + \gz_2} \\
&= \pa{x - n \frac{\Pi}{2} - n \pa{\gz_1 + \gd C_2 \; \gd_4}} \pa{1 + \gz_2} \\
&= x - n \frac{\Pi}{2} - n \pa{\gz_1 + \gd C_2 \; \gd_4} \pa{1 + \gz_2} + \pa{x - n \frac{\Pi}{2}} \; \gz_2
\end{align*}
from which we deduce an upper bound on the absolute error of the reduction, noting that $\abs{x - n \frac{\Pi}{2}} \leq \frac{\Pi}{4}\pa{1 + \gg_2 \pa{2^{\gk_2 + 1} + 1}}$ as per (\ref{eqnmisroundingbound}):
\begin{align*}
&\abs{y - y' - \gd y - \pa{x - n \frac{\Pi}{2}}} \\ 
& \qquad \leq 2^{\gk_2 + \gk'_2 + \gk''_2} \pa{2^{-2 M - 1} + 2^{-2 M} + 2^{-3 M - 1}} \pa{1 + 2^{2 - 2 M}} \ULP\of{\frac{\Pi}{2}} + 2^{2 - 2 M} \frac{\Pi}{4}\pa{1 + \gg_2 \pa{2^{\gk_2 + 1} + 1}} \\
& \qquad < 2^{\gk_2 + \gk'_2 + \gk''_2 - 2 M} \pa{\frac{3}{2} + 2^{-M - 1}} \pa{1 + 2^{2 - 2 M}} \ULP\of{\frac{\Pi}{2}} + 2^{-2 M} \; \Pi \pa{1 + 3 \times 2^{\gk_2} \ULP\of{\frac{\Pi}{2}}} \\
& \qquad < 2^{\gk_2 - 2 M} \pa{2^{\gk'_2 + \gk''_2 + 1} + 3} \ULP\of{\frac{\Pi}{2}} + 2^{-2 M} \; \Pi
\end{align*}
where the second inequality uses $\gg_2 \pa{2^{\gk_2 + 1} + 1} < 3 u \; 2^{\gk_2 + 1}$.

A sufficient condition for the reduction to guarantee $\gk_3$ extra bits of accuracy is for this error to be less than $2^{-\gk_3 - 1} \ULP\of{\red x}$ which itself is less than $2^{-\gk_3 - M} \abs{\red{x}}$.  Therefore we want:
\begin{align*}
\abs{\red x} &\geq 2^{\gk_3 - M} \pa{2^{\gk_2} \pa{2^{\gk'_2 + \gk''_2 + 1} + 3} \ULP\of{\frac{\Pi}{2}} + \Pi} \\
&= 2^{\gk_3 - M} \pa{2^{\gk_2 - M + 1} \pa{2^{\gk'_2 + \gk''_2 + 1} + 3} + \Pi}
\end{align*}
and it is therefore sufficient to have:
\[
\abs{\red x} \geq 2^{\gk_3 - M} \pa{2^{\gk_2 + \gk'_2 + \gk''_2 - M + 2} + 4}
\]

If we choose $\gk_3 = 18$ as above, and $\gk_2 = 18$ we find that $\gk'_2 = 14$ and $\gk''_2 = 15$.  Therefore, the desired accuracy is obtained as long as $\abs{\red x} \geq 65 \times 2^{-39} \simeq 1.2 \times 10^{-10}$.

\subsection*{Fallback}

If any of the conditions above is not met, we fall back on the CORE-MATH implementation.

\section*{Accurate Tables and Their Generation}

First, a remark on the notation.  This section (as well is the code in \linebreak\texttt{accurate\_table\_generator*.hpp}) follows the notation of \cite{StehléZimmermann2005}.  In particular, $N$ is related to the number of bits in the mantissa of the floating-point type, and $M$ is related to the number of zeroes or ones after the mantissa of the accurate values.  Specifically, since we use \texttt{binary64} and want 18 zeroes or ones after the mantissa (to minimize the probability of expensive fall backs to the CORE-MATH implementation), we have $N = 2^{53}$ and $M = 2^{18}$. 

The accurate tables are tables of triples $\pa{x_k, s_k, c_k}$ where $x_k$ is chosen such that $\sin x_k$ and $\cos x_k$ are very close to machine numbers, \idest, have a string of zeroes or ones after the last bit of their mantissa.  The tabulated value $s_k$ (respectively, $c_k$) is then obtained by rounding $\sin x_k$ (respectively, $\cos x_k$) to the nearest machine number.  See \nameref{overview} below for the details of how these tables are used for implementing $\sin$ and $\cos$.

The argument range (which is approximately $\intclos{0}{\frac{\Pi}{4}}$) is split into intervals of length $2 \gD$ centered on $2 k \gD$.  The interval\footnote{Obviously the intervals $\mathscr{I}_k$ cannot all contain their bounds.  Because of the way they are computed, the odd multiples of $\gD$ which separate the intervals are rounded to the nearest even $k$.  Therefore, there is an alternation of open-open intervals (for $k$ odd) and closed-closed intervals (for $k$ even).  We do not have a convenient notation for this, and anyway this detail is mostly irrelevant.} containing $x_k$ is $\mathscr{I}_k \DefineAs \intclos{\pa{2 k - 1}\gD}{\pa{2 k + 1}\gD}$ (for $k = 0$ the interval is $\mathscr{I}_0 \DefineAs \intclos{0}{\gD}$ and $x_0 = 0$).  We want to select $x_k$ as close as possible to the midpoint of $\mathscr{I}_k$, because we will need to approximate $\sin$ and $\cos$ over the interval $\intclos{-h_{\text{max}}}{h_{\text{max}}}$ with:
\[
h_{\text{max}} \DefineAs \max_k \pa{x_k - \pa{2 k - 1} \gD, \pa{2 k + 1} \gD - x_k}
\]
and we want this interval to be as small as possible to yield a more precise polynomial.

In practice we choose $\gD = 2^{-10}$ and the accurate tables construction yields $h_{\text{max}} < \gD + 2^{-17.834}$.  \marginnote{TODO(phl): Why $\gD$?  Number of perturbed bits?}

The naïve table construction method described in \cite{Gal1986} (random sampling over $\mathscr{I}_k$) has two problems.  First, it doesn't guarantee that $x_k$ is close to the midpoint of $\mathscr{I}_k$ and can in fact, in the worst case, result in $x_k$ being very close to one of the bounds of the interval, requiring the construction of a minimax polynomial over an interval which is (nearly) two times too big.  Second, for each triple of the accurate table it has to inspect, on average, around $M^2 = 2^{36}$ values.  On a modern processor, testing one value takes approximately $\SI{50}{\micro\second}$, which amounts to about 900 hours for each triple of the table, and therefore 43 years for the 402 triples that we need.

Consequently, we use the method described in \cite{StehléZimmermann2005} to build our accurate tables.  It is much faster than the method in \cite{Gal1986}, but must still be implemented with great care to achieve a satisfactory speed.  In the rest of this section, we describe the optimizations that we used to generate our tables efficiently.  On a modern processor (AMD Ryzen Threadripper PRO 5965WX) the entire table generation takes about 17 seconds.

\subsection*{The Stehlé-Zimmermann Algorithm}

For convenience, we copy algorithm \ref{SBCS} from \cite{StehléZimmermann2005}.
\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\LinesNumberedHidden
\SetNlSty{texttt}{}{.}
\SetAlgoNlRelativeSize{0}
\KwIn{Two functions $F_1$ and $F_2$, and two positive integers $M$, $T$.}
\SetKwFunction{IntegerRoots}{IntegerRoots}
\SetKwFunction{LatticeReduce}{LatticeReduce}
\KwOut{All $t \in \intclos{-T}{T}$ such that $\abs{F_i\of{t} \cmod 1} < \frac{1}{M}$ for $i \in \set{1, 2}$.}
\ShowLn Let $P_1\of{t}$, $P_2\of{t}$ be the degree-2 Taylor expansions of $F_1\of{t}$, $F_2\of{t}$.\;
\ShowLn Compute $\ge$ such that $\abs{P_i\of{t} - F_i\of{t}} < \ge$ for $\abs t \leq T$ and $i \in \set{1, 2}$.\;
\ShowLn Compute $M' = \floor{\frac{1/2}{1/M + \ge}}$, $C = 3 M'$, and\footnotemark{} $\tilde{P}_i\of{\gt} = \iround{C · P_i\of{T \gt}}$ for $i \in \set{1, 2}$.\;
\ShowLn Let $e_1 = 1$, $e_2 = \gt$, $e_3 = \gt^2$, $e_4 = \gu$, $e_5 = \gf$.\;
\ShowLn Let $g_1 = C$, $g_2 = C T · \gt$, $g_3 = \tilde{P}_1\of{\gt} + 3 \gu$, $g_4 = \tilde{P}_2\of{\gt} + 3 \gf$.\;
\ShowLn Create the $4 × 5$ integral matrix $L$ where $L_{k, l}$ is the coefficient of the monomial $e_l$ in $g_k$.\;
\ShowLn $V$ \leftarrow $\LatticeReduce\of{L}$.\;
\ShowLn Let $\vv_1$, $\vv_2$, $\vv_3$ be the three shortest vectors of $V$, and $Q_i\of{\gt}$ the associated polynomials.\;
\ShowLn \lIf{there exist $i \in \set{1, 2, 3}$ such that $\Lnorm[1]{\vv_i} \geq C$} {
  \Return{\texttt{FAIL}}.
}
\ShowLn Let $Q\of{\gt}$ be a linear combination of the $Q_i$'s which is independent of $\gu$ and $\gf$.
We have $\deg Q \leq 1$.\;
\ShowLn Let $q\of{t} = Q\of{\frac{t}{T}}$.\;
\lForEach{$t_0$ in $\IntegerRoots\of{q, \intclos{-T}{T}}$}{
  \lIf{$\abs{F_i\of{t_0} \cmod 1} < \frac{1}{M}$ for all $i \in \set{1, 2}$}{
    \Return{$t_0$.}
  }
}
\caption{SimultaneousBadCaseSearch\label{SBCS}.}
\end{algorithm}

\footnotetext{The notation $\iround{C · P_i\of{T \gt}}$ means that we round to the nearest each coefficient of $C · P_i\of{T \gt}$.  This gives an element of $\Integers [ \gt ]$. (This footnote is missing from \cite{StehléZimmermann2005} but present in the preprint \cite{StehléZimmermann2004}.)} 

When algorithm \ref{SBCS} returns a value at step 11, that value can be used to build an accurate table entry.  When it does not return a value at step 11, the interval $\intclos{-T}{T}$ is guaranteed not to contain a point suitable for building an accurate table entry.  When it fails at step 9, the algorithm must be retried over $\intclos{-T / 2}{T / 2}$ until a conclusive answer is found at step 11.  Note that, despite the phrase ``All $t$'' in the description of the result, algorithm \ref{SBCS} returns at most one value (because $\deg Q \leq 1$).

\subsection*{The Complete Search}

In order to construct the accurate we need to build on algorithm \ref{SBCS} to perform a search over a arbitrary interval.  In this context, if $T$ is small enough, we are better off doing an exhaustive search than going through the fairly expensive lattice reduction.  For an exhaustive search, we let the caller specify whether the search should proceed by increasing or decreasing values to make sure that we find, to the extent possible, the solution closest to the centre of $\mathscr{I}_k$.  Algorithm \ref{ISBCS} spells out the processing of one interval specifically for the $\sin$ and $\cos$ functions.

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\LinesNumbered
\SetNlSty{texttt}{}{.}
\SetAlgoNlRelativeSize{0}
\KwIn{The interval to search $\intclos{x_{\text{mid}} - \frac{t}{N}}{x_{\text{mid}} + \frac{t}{N}}$ defined by its midpoint $x_{\text{mid}}$ and its scaled radius $T$,
and a positive integer $M$.}
\KwOut{A $t \in \intclos{-T}{T}$ such that $x_{\text{mid}} + \frac{t}{N}$ is an accurate $x$-value for $\sin$ and $\cos$; furthermore, if the search direction is $\texttt{UP}$, $t$ is as close as possible to $-T$ and the search direction is $\texttt{DOWN}$, $t$ is as close as possible to $T$.}
\SetKwFunction{SimultaneousBadCaseSearch}{SimultaneousBadCaseSearch}
Let $F_1 = \FunctionBody{t}{N \sin\of{x_{\text{mid}} + \frac{t}{N}}}$ and $F_2 = \FunctionBody{t}{N \cos\of{x_{\text{mid}} + \frac{t}{N}}}.$\;
\uIf{$T$ < some threshold}{
  \Switch{search direction}{
    \uCase{$\texttt{UP}$}{
    	  \ForEach{$t_0 \in \intclos{-T}{T}$ by ascending values}{
        \If{$\abs{F_i\of{t_0} \cmod 1} < \frac{1}{M}$ for all $i \in \set{1, 2}$}{
          \Return{$t_0$.}
        }
      }
    }
    \uCase{$\texttt{DOWN}$}{
    	  \ForEach{$t_0 \in \intclos{-T}{T}$ by descending values}{
        \If{$\abs{F_i\of{t_0} \cmod 1} < \frac{1}{M}$ for all $i \in \set{1, 2}$}{
          \Return{$t_0$.}
        }
      }
    }
  }
}
\Else{
  \Return \SimultaneousBadCaseSearch{$F_1$, $F_2$, $M$, $T$}.\;
}
\caption{IntervalSimultaneousBadCaseSearch\label{ISBCS}.}
\end{algorithm}

Algorithm \ref{ISBCS} must be integrated into a retry loop that searches progressively farther above and below $2 k \gD$ until a solution is found.  Algorithm \ref{FSBCS} performs such a complete search.  It first searches an interval of radius $T_0$ above $2 k \gD$, and retries with intervals progressively smaller if algorithm \ref{ISBCS} returns a failure.  When algorithm \ref{ISBCS} returns an empty solution set, the last interval searched is marked as ``covered'' and the algorithm proceeds to perform a similar search below $2 k \gD$.  If that search does not find a solution, a new interval of radius $T_0$ above the covered interval is tried, and so on iteratively.  The algorithm returns when a solution is found.  That solution is nearly optimal in terms of its distance to $2 k \gD$ with the caveat that, since we search above $2 k \gD$ before searching below, we could return a solution above when there is a slightly closer solution below.

The value of $T_0$ is chosen based on the Coppersmith bound established in \cite{StehléZimmermann2005}: $T^3 = \BigO\of{M · N}$.  Evidently this bound does not unambiguously determine $T_0$, but in practice $T_0 = \sqrt[3]{M N}$ yields good results: a value too large would cause too many splits of the search interval before finding a solution or (more likely) rejecting the interval; a value too small would require to process too many small intervals. 

\begin{algorithm}
\DontPrintSemicolon
\SetAlgoLined
\LinesNumbered
\SetNlSty{texttt}{}{.}
\SetAlgoNlRelativeSize{0}
\KwIn{$\gD$, the half-size of intervals, and $k$, the index of the interval to search.}
\KwOut{A $t$ sufficently close to zero}
\SetKw{Break}{break}
\SetKwFor{Loop}{loop}{}{end}
\SetKwFunction{IntervalSimultaneousBadCaseSearch}{IntervalSimultaneousBadCaseSearch}
\SetKwFunction{Midpoint}{Midpoint}
$T_0$ \leftarrow $\sqrt[3]{M N}$\;
$\mathscr{I}_{\text{covered}}$ \leftarrow $\intclos{2 k \gD}{2 k \gD}$\;
\Loop{}{
  $T$ \leftarrow $T_0$\;
  \Loop{}{
    Let $\mathscr{I}_{\text{to\_cover}}$ be the interval of radius $T$ immediately above $\mathscr{I}_{\text{covered}}$.\;
    $t$ \leftarrow \IntervalSimultaneousBadCaseSearch{$\mathscr{I}_{\text{to\_cover}}$, $M$}.\;
    \uIf{$t$ is a solution}{
      \Return{\Midpoint{$\mathscr{I}_{\text{to\_cover}}$} $+ \frac{t}{N}$}.\;
    }
    \uElseIf{$y$ = \texttt{FAIL}}{
      $T$ \leftarrow $T / 2$.\;
    }
    \Else{
      $\mathscr{I}_{\text{covered}}$ = $\mathscr{I}_{\text{covered}} \cup \mathscr{I}_{\text{to\_cover}}$\;
      \Break\;
    }
  }
  \Loop{}{
    Let $\mathscr{I}_{\text{to\_cover}}$ be the interval of radius $T$ immediately below $\mathscr{I}_{\text{covered}}$.\;
    $t$ \leftarrow \IntervalSimultaneousBadCaseSearch{$\mathscr{I}_{\text{to\_cover}}$, $M$}.\;
    \uIf{$t$ is a solution}{
      \Return{\Midpoint{$\mathscr{I}_{\text{to\_cover}}$} $+ \frac{t}{N}$}.\;
    }
    \uElseIf{$y$ = \texttt{FAIL}}{
      $T$ \leftarrow $T / 2$.\;
    }
    \Else{
      $\mathscr{I}_{\text{covered}}$ = $\mathscr{I}_{\text{covered}} \cup \mathscr{I}_{\text{to\_cover}}$\;
      \Break\;
    }
  }
}
\caption{FullSimultaneousBadCaseSearch\label{FSBCS}.}
\end{algorithm}

\subsection*{Representation of Functions and Polynomials}

Our implementation is based on the Boost \texttt{multiprecision} library.  The functions $\sin$ and $\cos$ take \texttt{cpp\_rational} values as arguments and return \texttt{cpp\_bin\_float\_50} values (50 decimal digits should be plenty for our purposes since we are looking for values that are accurate to approximately 21 digits; at any rate, while the construction of the accurate values is expensive, checking that they are correct is straightforward so we are not concerned about accuracy issues in this context).  The polynomial approximations take \texttt{cpp\_rational} values as arguments, have coefficients that are \texttt{cpp\_rational} and return \texttt{cpp\_rational}.

Step 2 of algorithm \ref{SBCS} requires that we compute an upper bound for $\abs{P_i\of{t} - F_i\of{t}}$ over an interval.  To do this efficiently (and in particular, without doing multiple evaluation of the functions and the polynomials), we approximate $P_i\of{t} - F_i\of{t}$ by the degree-3 term of the Taylor expansion of $F_i\of{t}$ and find the extremum of this term.  While ignoring the terms of degree 4 and higher introduces a small error, this error is of no importance as $\ge$ is typically much smaller than $1 / M$ and does not affect the value of $M'$ (see step 3).

It would be possible (and correct) to compute the polynomials $P_i$ using a Taylor approximation around $2 k \gD$ and to translate them just like we translate the functions in algorithm \ref{ISBCS}: $\FunctionBody{t}{N P_i\of{x_{\text{mid}} + \frac{t}{N}}}$.  In practice however this results in polynomial coefficients with very large numerators and denominators, greatly affecting the performance of algorithm \ref{SBCS}.  It is much more efficient to recompute the Taylor expansion around $x_{\text{mid}}$ as this generates coefficients with smaller numerators and denominators for the same accuracy.  (The coefficients of the Taylor expansion involve evaluations of $\sin$ and $\cos$, so the two methods do not generally result in the exact same polynomial.)

\subsection*{Lattice Reduction}

Even though the matrix $L$ is small, a fair amount of time is spent in the lattice reduction at step 7 of algorithm \ref{SBCS}.  The classical algorithm for lattice reduction is the Lenstra-Lenstra-Lovász algorithm, described for instance in \cite[444]{HoffsteinPipherSilverman2014} but it turns out to be fairly costly in our case because the matrix elements are of type \texttt{cpp\_int} and operations on this type are expensive.  Instead we use the algorithm described in \cite{NguyễnStehlé2009}, which is quadratic and speeds up the search significantly.

\subsection*{Speculative Execution}

The natural way to parallelize the search on a multicore processor is to perform the searches for different intervals $\mathscr{I}_k$ in their own thread.  Unfortunately this does not work very well because the searches have a ``long tail'': for most intervals, a solution is found relatively quickly, but for some intervals the search has to explore values far from $2 k \gD$.  So after a short while only a few cores are used, effectively doing a sequential search over ``expensive'' intervals, and most of the cores are idle.  This does not properly take advantage of the available paralellism.

It is much more effective to parallelize using speculative execution as follows.  Each of the intervals of radius $T_0$ processed in steps 5 and 17 of algorithm \ref{FSBCS} is a \emph{slice} and slices can be searched independently for a given value of $k$.  As soon as a core is available, it picks a slice to process, which is guaranteed not to have been processed already, but which may not be adjacent to the interval $\mathscr{I}_{\texttt{covered}}$ (it is centered at $2 k \gD + \pa{2 j + 1} \frac{T_0}{N}$ where $j$ is the \emph{slice index}).  The execution is speculative in the sense that the search for slice index $j$ may not be necessary if a solution exists, say, for index $j - 1$.  However, most slices do not contain a solution, so the amount of wasted work is minimal.  Note that, for speculative execution to be deterministic, it is important, if a solution is found in slice $j$, to wait for the searches with indices preceding $j$ to complete before returning the solution, as one of these slices may contain a solution closer to $2 k \gD$.

\section*{Overview of the Algorithms}\label{overview}

This section presents an overview of the algorithms used to compute $\sin$ and $\cos$.  They take as input the result of argument reduction, $\pa{\red x, \gd \red x}$ and produce a pair $\pa{y, \gd y}$ which is passed to the rounding test described in \cite[397]{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010} to decide whether $\round{y + \gd y}$ is the correctly-rounded result (this is the case with high probability) or whether we need to fall back to the CORE-MATH implementation.

The lower part of the reduced angle, $\gd \red x$, must be used with care: on the one hand, it would not make sense to explicitly compute expressions like $\round{\red x + \gd \red x}$ as the second term vanishes before the first one; on the other hand, we must avoid the computation of expressions involving $\gd \red x$ that have no bearing on the final result (see \cite[402-404]{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010} for a discussion of this issue).  We are going to spell out the places where $\gd \red x$ is actually used and we will prove later that neglecting it in other places has no effect.

\subsection*{Sin Near Zero}

For the $\sin$ function near zero the accurate tables method is not usable because the correction term is not small compared to the tabulated value of the function (which can be arbitrarily close to zero).  Instead we use a polynomial approximation that minimizes the relative error on the result.  Since $\sin t$ is an odd function and since its dominant term is $t$, we are using an approximation of the form:
\[
\sin t \simeq t + t^3 \; p_{s0}\of{t^2}
\]
and we compute:
\[
\sin x \simeq \sin\of{\red x + \gd \red x} \simeq \red x + {\red x}^3 p_{s0}\of{{\red x}^2} + \gd \red x
\]

\subsection*{Sin and Cos Around Table Entries}

Let $\pa{x_k, s_k, c_k}$ be an accurate table entry.  The implementation of $\sin$ and $\cos$ starts by choosing, from the argument $\red x$ (which in the case of $\sin$, is not close to zero), a $k$ such that $\red x \in \mathscr{I}_k$ and computing $h = \red x - x_k$.  We then use the addition formulæ to separate out the terms in $x_k$:
\begin{align*}
\sin x &\simeq \sin\of{\red x + \gd \red x} \\
&= \sin\of{x_k + h + \gd \red x} \\
&= \sin x_k \; \cos\of{h + \gd \red x} + \cos x_k \; \sin\of{h + \gd \red x} \\
\cos x &\simeq \cos\of{\red x + \gd \red x}  \\
&= \cos\of{x_k + h + \gd \red x} \\
&= \cos x_k \; \cos\of{h + \gd \red x} - \sin x_k \; \sin\of{h + \gd \red x}
\end{align*}
The quantities $\sin x_k$ and $\cos x_k$ are extremely close to $s_k$ and $c_k$ (this is the core of the accurates table method).  The terms in $h + \gd \red x$ may be approximated by polynomials chosen to respect the parity of $\sin$ and $\cos$ and their values at $0$:
\begin{align*}
\sin t &\simeq t + t^3 \; p_s\of{t^2} \\
\cos t &\simeq 1 + t^2 \; p_c\of{t^2}
\end{align*}
After neglecting the terms in $\gd \red x$ that do not contribute to the final result we obtain\footnote{The alert reader will note that the formulæ for $\sin$ and $\cos$ are very similar, which creates an opportunity for implementing a function that computes both together with significant performance savings.}:
\begin{align*}
\sin x &\simeq s_k \pa{1 + h \pa{h + 2 \; \gd \red x} \; p_c\of{h^2}} + c_k \pa{h + \gd \red x + h^3 \; p_s\of{h^2}} \\
&= \pa{s_k + c_k h} + s_k h \pa{h + 2 \; \gd \red x} \; p_c\of{h^2} + c_k h^3 \; p_s\of{h^2} + c_k \gd \red x \\
\cos x &\simeq c_k \pa{1 + h \pa{h + 2 \; \gd \red x} \; p_c\of{h^2}} - s_k \pa{h + \gd \red x + h^3 \; p_s\of{h^2}} \\
&= \pa{c_k - s_k h} + c_k h \pa{h + 2 \; \gd \red x} \; p_c\of{h^2} - s_k h^3 \; p_s\of{h^2} - s_k \gd \red x
\end{align*}
where $h \pa{h + 2 \; \gd \red x}$ is an approximation of $\pa{h + \gd \red x}^2$.  For accuracy reasons, the leading term of these formulæ must be computed with extra accuracy.

We are now going to look at the techniques used to obtain polynomial approximation and we will later analyze the errors committed by these formulæ.

\section*{Polynomial Approximations}

The \textit{Mathematica} function \texttt{GeneralMiniMaxApproximation} produces a minimax polynomial $p$ such that $p\of{q\of{t}}$ approximates a function $f\of{t}$ by minimizing the $\Lspace$ norm of the \emph{residual} function defined as:
\[
\mathscr{E}\of{t} \DefineAs \frac{f\of{t} - p\of{q\of{t}}}{g\of{t}}
\]
The residual is equioscillatory and bounded.  The choice of $f$ and $q$ is dictated by the properties desired for the polynomial approximation: parity, value at zero, etc.  The error function $g$, however, can be freely chosen to achieve the desired error bound on the approximation (\exempligratia, minimizing the relative or absolute error).

\subsection*{Sin Near Zero}

Near zero we are looking for an approximation that minimizes the relative error of the result over the interval $\intclos{0}{\gD}$, where $\gD$ is chosen so that $\gD^2 \ll 1$.

We are therefore calling \texttt{GeneralMiniMaxApproximation} with:
\[
\begin{dcases}
q\of{t} &\DefineAs t^2 \\
f\of{t} &\DefineAs \frac{\sin t - t}{t^3} \\
g\of{t} &\DefineAs \frac{\sin t}{t^3}
\end{dcases}
\]
The degree of $p_{s0}$ is choosen so that the approximation error is less than $\ULP\of{\gD^2}$.

In practice we choose $\gD = 2^{-10}$, and compute a degree-1 polynomial with a residual smaller than $2^{-85.560}$ (before rounding the coefficients to machine numbers).

\subsection*{Sin and Cos Around Table Entries}

Around table entries, it would be natural to approximate $\cos$ by a polynomial minimizing the absolute or relative error.  Unfortunately this creates singularities in the error analysis.  To see why, consider the term $h \pa{h + 2 \; \gd \red x} p_c\of{h^2}$ which appears in the computations above, and expand it by making the error function $g$ and the residual $\mathscr{E}$ explicit:
\begin{align*}
h \; \pa{h + 2 \; \gd \red x} p_c\of{h^2} &= h \; \pa{h + 2 \; \gd \red x} \pa{\frac{\cos h - 1}{h^2} + g\of{h} \mathscr{E}\of{h}} \\
&= \cos h - 1 + 2 \; \gd \red x \frac{\cos h - 1}{h} + 2 \; h \; \gd \red x \; g\of{h} \mathscr{E}\of{h} + h^2 g\of{h} \mathscr{E}\of{h}
\end{align*}
When $h \to 0$, the dominant term is $2 \; h \; \gd \red x \; g\of{h} \mathscr{E}\of{h}$.  Since $\mathscr{E}\of{h}$ is bounded, this term is of order $\BigO\of{h \; g\of{h}}$.  To obtain an absolute or relative error on $\cos$ we would need to choose $g\of{h} = 1/h^2$ or $g\of{h} = \cos h/h^2$, respectively.  However, these choices yield a dominant term of order $\BigO\of{1/h}$ which diverges when $h \to 0$.  The root cause is that we are computing an approximation based on $h$ alone, but we use it in combination with the term in $\gd \red x$ which was not part of the optimization.

To avoid this issue, we choose $g\of{h} = \pa{\cos h - 1}/h^2$ which minimizes the relative error on $p_c\of{h}$ and leads to a straightforward error analysis.  Therefore, for $\cos$ we call \texttt{GeneralMiniMaxApproximation} with:
\[
\begin{dcases}
q\of{t} &\DefineAs t^2 \\
f\of{t} &\DefineAs \frac{\cos t - 1}{t^2} \\
g\of{t} &\DefineAs \frac{\cos t - 1}{t^2}
\end{dcases}
\]

For $\sin$ there is no such issue, therefore we simply minimize the relative error on $\sin t$:

\[
\begin{dcases}
q\of{t} &\DefineAs t^2 \\
f\of{t} &\DefineAs \frac{\sin t - t}{t^3} \\
g\of{t} &\DefineAs \frac{\sin t}{t^3}
\end{dcases}
\]

The minimax computation results in a 1-degree polynomial $p_s$ with with a residual smaller than $2^{-85.534}$ and a 1-degree polynomial $p_c$ with a residual smaller than $2^{-51.466}$ (before rounding the coefficients to machine numbers).

\section*{Error Analysis}

We use $\roundAll{expr}$ to denote evaluation where appropriate rounding happens on each literal or operation of the expression $expr$.  This notation is used for error intervals computed using the function \texttt{IEEEEvaluateWithRelativeError} in file {\linebreak}\href{https://github.com/mockingbirdnest/Principia/blob/master/mathematica/ieee754_floating_point_evaluation.wl}{\texttt{mathematica/ieee754\_floating\_point\_evaluation.wl}}.  In particular, $\roundAll{\;}$ takes into account that the evaluation of the polynomials $p_{s0}$, $p_s$, and $p_c$ has two sources of errors: the rounding of the coefficients to machine numbers, and the error due to the floating-point operations; this may lead to asymmetrical error intervals. \marginnote{TODO(phl): Try to tune the high-degree coefficient after rounding the low-degree one.}

We assume that $\red x$ is positive ($\gd \red x$ may be positive or negative) and that the rounding direction is roundTiesToEven.  We \emph{do not} assume that the machine has an FMA instruction in our error analysis: even though we actually use this instruction when available (for performance) our rounding bounds are valid even in the absence of an FMA\footnote{This means that the accuracy computed below is possibly pessimistic by a few hundredth of bits, which has no effect on performance.}.

\subsection*{Reduced Angle}

Argument reduction took the input angle $x$ and produced a pair $\pa{\red x, \gd \red x}$ approximating the angle reduced modulo $\frac{\Pi}{2}$.  That approximation is correct to $M + \gk_3$ bits.  We therefore have, from equation (\ref{eqnreductionbound}):
\[
x \equiv \red x + \gd \red x + \gz_0 \; \red x \pmod{\frac{\Pi}{4}} \qquad \abs{\gz_0} \le 2^{-\gk_3 - M}
\]
For simplicity of the error analysis we can assume $0 \le x \le \frac{\Pi}{4}$: even though we do not do argument reduction when $x$ is less than $\frac{\Pi}{4}$, the error analysis would work exactly the same if $x$ was simply provided as a high-accuracy value (\exempligratia, a double-double) and reduced using the techniques above.

\subsection*{Rounding Test}

The rounding test described in \cite[397-400]{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010} is done by comparing $y$ and $\round{y + \round{\gd y \; e}}$ for equality, where $e > 1$ is computed based on the bound on the relative error of $y + \gd y$ with respect to $\sin x$ or $\cos x$.  In our case, when an FMA is available, we want to compute the second part as $\round{y + \gd y \; e}$.  We must analyse what this implies for the computation of $e$.

The proof of the rounding test is rather convoluted, but for our purpose the important part is the implication\footnote{We adapt the proof to use our notation.}, when $y$ is not a power of 2 or $\gd y \geq 0$:
\[
y = \round{y + \round{\gd y \; e}} \implies \round{\gd y \; e} \le \frac{\ULP\of{y}}{2} \implies \gd y \; e \; \pa{1 - \ULP\of{\frac{1}{2}}} \le \frac{\ULP\of{y}}{2}
\]
When an FMA is used the smaller term has to be less that half a ULP of the larger term, otherwise the result would round away from $y$.  This gives the tighter bound:
\[
y = \round{y + \gd y \; e} \implies \gd y \; e \le \frac{\ULP\of{y}}{2}
\]

Similarly when $y$ is a power of 2 and $\gd y < 0$ the proof depends on the implications:
\[
y = \round{y + \round{\gd y \; e}} \implies \abs{\round{\gd y \; e}} \le \frac{\ULP\of{y}}{4} \implies -\gd y \; e \; \pa{1 - \ULP\of{\frac{1}{2}}} \le \frac{\ULP\of{y}}{4}
\]
When an FMA is used the smaller term, which is negative, has to be less than half a ULP \emph{below} the larger term otherwise the result would round below $y$.  Because the ULP below is half the ULP above, this gives the tighter bound:
\[
y = \round{y + \gd y \; e} \implies -\gd y \; e \le \frac{\ULP\of{y}}{4}
\]

Putting these tighter bounds together we obtain:
\begin{align*}
k &\DefineAs \floor{-\log_2{\overbar\ge_1} - M} \\
e_{\text{FMA}} &\DefineAs 1 + \frac{1 + 2^{M + 1} \; \overbar\ge_1}{1 - \overbar\ge_1 - 2^{-k + 1}}
\end{align*}

In the absence of FMA, \cite{MullerBrisebarreDeDinechinJeannerodLefevreMelquiondRevolStehleTorres2010} gives the same value for $k$ and for $e$:
\[
e_{\text{NoFMA}} \DefineAs \pa{1 - 2^{-M}}^{-1}\pa{1 + \frac{1 + 2^{M + 1} \; \overbar\ge_1}{1 - \overbar\ge_1 - 2^{-k + 1}}}
\]
We see immediately that $e_{\text{FMA}} < e_{\text{NoFMA}}$, and it is therefore correct to perform the rounding test using $e_{\text{NoFMA}}$, even if an FMA is used to compute $\round{y + \gd y \; e}$.

Obviously, if the above expression does not produce a machine number, it must be rounded towards $+\infty$.  In each of the error analyses below we document the concrete factor $\roundTowardPositive{e_{\text{NoFMA}}}$ to use for the rounding test.

\subsection*{Sin Near Zero}\label{secsinnearzero}

If $\abs{\red x} \le \gD = 2^{-10}$ the steps of the computation are as follows:
\[
\begin{dcases}
t_1 &\DefineAs \roundAll{p_{s0}\of{{\red x}^2}} \\
t_2 &\DefineAs \round{\round{{\red x}^2} \; \red x} \\
t_3 &\DefineAs \round{\round{t_1 t_2} + \gd \red x} \\
y &\DefineAs \red x \\
\gd y &\DefineAs t_3
\end{dcases}
\]

We gave above the upper bound of the residual of the minimax approximation.  It may be rewritten as:
\[
p_{s0}\of{t^2} = \frac{\pa{1 + \gz_1} \sin t - t}{t^3} \qquad \abs{\gz_1} < 2^{-85.560}
\]
for $\abs t \le \gD$.

The errors committed at each step are as follows:
\[
\left\{
\begin{alignedat}{3}
t_1 &= p_{s0}\of{{\red x}^2} \pa{1 + \gz_2} &\gz_2 &\in \intopen{-2^{-52.415}}{2^{-53.999}} \\
&= \frac{\pa{1 + \gz_1} \sin {\red x} - x}{{\red x^3}} \pa{1 + \gz_2} \\
t_2 &= {\red x}^3 \pa{1 + \gd_1} \pa{1 + \gd_2} \\
t_3 &= \pa{t_1 t_2 \pa{1 + \gd_3} + \gd \red x} \pa{1 + \gd_4}
\end{alignedat}
\right.
\]
The relative error of the entire computation is:
\[
r\of{\red x, \gd \red x} \DefineAs \frac{y + \gd y}{\sin\of{\red x + \gd \red x + \gz_0 \; \red x}} - 1
\]
The function $r$ can be computed using \textit{Mathematica} interval arithmetic over the triangular domain $\abs{\red x} \le \gD$, $\abs{\gd \red x} \le 2^{-M} \abs{\red x}$.  Plotting shows\marginnote{TODO(phl): See if this can be proven rigourously.} that this function reaches its extrema at the corners of the domain, which is logical because we expect the minimax polynomial to reach its largest errors on the bounds of the optimization interval.  In practice we find that $\abs{r\of{\red x, \gd \red x}} < 2^{-70.583}$ (as expected, this is a bit worse than the error originating from the angle reduction).  The rounding test must be done with $e = \texttt{0x1.0000'AAD0'391A'Dp0}.$\marginnote{TODO(phl): $\gk_3 = 19$ would gain 0.7 bits.  Worthwhile?}

As mentioned above (see \nameref{overview}), in these steps we do not compute the terms ${\red x}^n \; \gd \red x$ for $n > 0$.  The largest such term is for $n = 2$ and the relative error that it induces is:
\[
-\frac{1}{2} \frac{{\red x}^2 \; \gd \red x}{\sin\of{\red x + \gd \red x}}
\]
This function reaches its extrema on the corners of the triangular domain and is found to be smaller than $2^{-73.999}$, so its contribution to the overall relative error would be very small.

\subsection*{Sin and Cos Around Table Entries}

If $\red x \in \mathscr{I}_k$ we first compute $h = \red x - x_k$.  It is essential that this subtraction be exact, and therefore that the conditions of Sterbenz's lemma be met: we must have $\red x / 2 \le x_k \le 2 \red x$.  Based on the range of $\red x$, a sufficient condition for this is:
\[
\frac{2 k + 1}{2} \gD \le x_k \le 2 \pa{2 k - 1} \gD
\]
Because $x_k$ is close to $2 k \gD$ the left part of this condition is trivially met for all $k > 0$.  However the second part is only trivially met if $k > 1$.  If $k = 1$ it becomes $x_1 \le 2 \; \gD$.  In other words, when building the accurate tables we must only look for $x_1$ \emph{below} $2 \; \gD$.  This is done by skipping the loop at step 5 of algorithm \ref{FSBCS}.

For $\sin$ we do not use the interval $\mathscr{I}_0$ (see \nameref{secsinnearzero}, above) but for $\cos$ we do.  That interval is special because $x_0 = 0$: the computation of $h$ is therefore trivially correct.

Interestingly, $x_1 < 2 \gD$ is also the necessary condition to compute $h \; c_k + s_k$ exactly using an FMA as explained in section 2.1 of \cite{StehléZimmermann2005}.  The subtraction in the second step of their algorithm must be exact, therefore the following Sterbenz inequalities must hold\footnote{Note that $\mathscr{I}_0$ is uninteresting because $c_0 = 1$ and $s_0 = 0$ so the computation is trivially exact.}:
\[
\frac{s_k}{2} \le h \; c_k + s_k \le 2 \; s_k
\]
This may be rewritten as:
\[
-\frac{1}{2} \tan x_k \le h \le \tan x_k
\]
Let $x_k = 2 k \gD + \ge_k$ for some small $\ge_k$.  By the definition of $h$ we have:
\[
-\gD - \ge_k \le h \le \gD - \ge_k
\]
Now we know that $x_k < \tan x_k$ and since $\gD - \ge_k < 2 k \gD + \ge_k$ for all $k > 0$ the right side of the Sterbenz condition is always true.  The left side is more interesting though as:
\[
-\frac{1}{2} \tan x_k < -\frac{x_k}{2} = -k \gD - \frac{\ge_k}{2}
\]
For $k > 1$, this quantity is clearly smaller than $-\gD - \ge_k$, the lower bound of $h$.  However, when $k = 1$, $-\gD - \ge_1 / 2 \le -\gD - \ge_1$ requires that $\ge_1 \le 0$, in other words, that $x_1$ be below $2 \gD$.  Note that we do not need to go through a similar proof for the exactness of $-h \; s_k + c_k$ because $h \; s_k$ is much smaller than $c_k$ for all $k$.

Finally, for the purpose of error analysis, the relative errors of the minimax polynomials may be rewritten as:
\begin{align*}
p_s\of{t^2} &= \frac{\sin t \; \pa{1 + \gz_1} - t}{t^3} \qquad & \abs{\gz_1} < 2^{-85.534} \\
p_c\of{t^2} &= \frac{\cos t - 1}{t^2} \pa{1 + \gz_2} \qquad & \abs{\gz_2} < 2^{-51.466}
\end{align*}

\subsubsection*{Sin}\label{secerroranalysissin}

The first step of the computation is to evaluate $h \; c_k + s_k$ exactly using an FMA:\marginnote{TODO(phl): Prove that this works, and examine what happens when we don't have FMA.}
\[
\pa{z, \gd z} = t_0 = h \; c_k + s_k
\]
where $z$ and $\gd z$ have nonoverlapping significands.  For the purpose of describing the computation and analysing errors we will write $\gd z = t_0 \; \gd_0$ and $z = t_0 \pa{1 - \gd_0}$, where the two terms are exact.

The remaining steps of the computation are then as follows:
\[
\begin{dcases}
t_1 &\DefineAs \roundAll{p_s\of{h^2}} \\
t_2 &\DefineAs \roundAll{p_c\of{h^2}} \\
t_3 &\DefineAs \round{h \round{h + \pa{\gd \red x + \gd \red x}}} \\
t_4 &\DefineAs \round{\round{h^2} \; h} \\
t_5 &\DefineAs \round{\round{\round{s_k} \; t_3} \; t_2} \\
t_6 &\DefineAs \round{\round{t_4 \; t_1} + \gd \red x} \\
t_7 &\DefineAs \round{\round{\round{c_k} \; t_6} + t_5} \\
t_8 &\DefineAs \round{\round{t_0 \; \gd_0} + t7} \\
y &\DefineAs t_0 \pa{1 - \gd_0} \\
\gd y &\DefineAs t_8
\end{dcases}
\]
where we have made the rounding of the accurate table elements $s_k$ and $c_k$ explicit, and used the fact that the computation of $\gd \red x + \gd \red x$ is exact.

The errors commited at each step are as follows:
\[
\left\{
\begin{alignedat}{2}
t_1 &= p_s\of{h^2} \pa{1 + \gz_3} \qquad & \gz_3 \in \intopen{-2^{-53.221}}{2^{-52.808}} \\
&= \frac{\sin h \; \pa{1 + \gz_1} - h}{h^3} \pa{1 + \gz_3} \\
t_2 &= p_c\of{h^2} \pa{1 + \gz_4} \qquad & \gz_4 \in \intopen{-2^{-52.855}}{2^{-53.160}} \\
&= \frac{\cos h - 1}{h^2} \pa{1 + \gz_2} \pa{1 + \gz_4} \\
t_3 &= h \pa{h + 2 \; \gd \red x} \pa{1 + \gd_1} \pa{1 + \gd_2} \\
t_4 &= h^3 \pa{1 + \gd_3} \pa{1 + \gd_4} \\
t_5 &= \round{s_k} \; t_3 \; t_2 \; \pa{1 + \gd_5} \pa{1 + \gd_6} \\
t_6 &= \pa{t_4 \; t_1 \; \pa{1 + \gd_7} + \gd \red x} \pa{1 + \gd_8} \\
t_7 &= \pa{\round{c_k} \; t_6 \; \pa{1 + \gd_9} + t_5} \pa{1 + \gd_{10}} \\
t_8 &= \pa{t_0 \; \gd_0 \; \pa{1 + \gd_{11}} + t_7} \pa{1 + \gd_{12}} \\
\end{alignedat}
\right.
\]
The relative error of the entire computation is:
\[
r\of{\red x, \gd \red x} \DefineAs \frac{y + \gd y}{\sin\of{\red x + \gd \red x + \gz_0 \; \red x}} - 1
\]

Great care is required when evaluating this expression using interval arithmetic because the terms in $\gd_0$ in $t_8$ and $y$ could be considered independent and lead to a useless bound.  To avoid this issue $\gd_0$ must be factored out to appear only once.  Specifically the $\gd_0$ term after factoring is:
\[
\pa{h \round{c_k} + \round{s_k}} \pa{\pa{1 + \gd_{11}} \pa{1 + \gd_{12}} - 1}
\]
which is pleasantly small, and even though $\gd_{12}$ also appears in the term that does not involve $\gd_0$, the effect of replicating it is acceptable.

The function $r$ must be evaluated separately for each interval around $x_k$, with $\red x \in \intclos{\pa{2 k - 1}\gD}{\pa{2 k + 1}\gD}$ and $\abs{\gd \red x} < {\abs{\red x}} / 2^M$.  This defines a trapezoidal domain and $r$ reaches its extrema at the corners of that domain.  In practice we find using \textit{Mathematica} interval arithmetic that $\abs{r\of{\red x, \gd \red x}} < 2^{-68.734}$.  The relative error for $k = 1$ is particularly high and is an outlier\marginnote{TODO(phl): Try to understand why.}.  Other than than, the worst relative errors correspond to table entries that have many ones after their accurate zeroes (or many zeroes after their accurate ones).  The rounding test must be done with $e = \texttt{0x1.0002'6752'8572'Dp0}$.\marginnote{TODO(phl): Should we try accurate tables with 19 bits?}

As mentioned above (see \nameref{overview}), in these steps we do not compute the terms $h^n \; \gd \red x$ for $n > 1$.  The largest such term is for $n = 2$ and the relative error that it induces is:
\[
-\frac{1}{2}\frac{h^2 \; \gd \red x \; \cos x_k}{\sin\of{\red x + \gd \red x}}
\]
This function reaches its extrema on the corners of the trapezoidal domain and is found to be smaller than $2^{-73.999}$ for all $k$, so its contribution to the overall relative error would be very small.  (The term for $n = 1$, on the other hand, induces an error of the order of $2^{-63.348}$ so it must be computed.)

\subsubsection*{Cos}\label{secerroranalysiscos}

The first step of the computation is to evaluate $-h \; s_k + c_k$ exactly using an FMA:\marginnote{TODO(phl): Prove that this works, and examine what happens when we don't have FMA.}
\[
\pa{z, \gd z} = t_0 = -h \; s_k + c_k
\]
where $z$ and $\gd z$ have nonoverlapping significands.  For the purpose of describing the computation and analysing errors we will write $\gd z = t_0 \; \gd_0$ and $z = t_0 \pa{1 - \gd_0}$, where the two terms are exact.

The remaining steps of the computation are then as follows:
\[
\begin{dcases}
t_1 &\DefineAs \roundAll{p_s\of{h^2}} \\
t_2 &\DefineAs \roundAll{p_c\of{h^2}} \\
t_3 &\DefineAs \round{h \round{h + \pa{\gd \red x + \gd \red x}}} \\
t_4 &\DefineAs \round{\round{h^2} \; h} \\
t_5 &\DefineAs \round{\round{\round{c_k} \; t_3} \; t_2} \\
t_6 &\DefineAs \round{\round{t_4 \; t_1} + \gd \red x} \\
t_7 &\DefineAs \round{-\round{\round{s_k} \; t_6} + t_5} \\
t_8 &\DefineAs \round{\round{t_0 \; \gd_0} + t_7} \\
y &\DefineAs t_0 \pa{1 - \gd_0} \\
\gd y &\DefineAs t_8
\end{dcases}
\]
where we have made the rounding of the accurate table elements $s_k$ and $c_k$ explicit, and used the fact that the computation of $\gd \red x + \gd \red x$ is exact.

The errors commited at each step are as follows:
\[
\left\{
\begin{alignedat}{2}
t_1 &= p_s\of{h^2} \pa{1 + \gz_3} \qquad & \gz_3 \in \intopen{-2^{-53.221}}{2^{-52.808}} \\
&= \frac{\sin h \; \pa{1 + \gz_1} - h}{h^3} \pa{1 + \gz_3} \\
t_2 &= p_c\of{h^2} \pa{1 + \gz_4} \qquad & \gz_4 \in \intopen{-2^{-52.855}}{2^{-53.160}} \\
&= \frac{\cos h - 1}{h^2} \pa{1 + \gz_2} \pa{1 + \gz_4} \\
t_3 &= h \pa{h + 2 \; \gd \red x} \pa{1 + \gd_1} \pa{1 + \gd_2} \\
t_4 &= h^3 \pa{1 + \gd_3} \pa{1 + \gd_4} \\
t_5 &= \round{c_k} \; t_3 \; t_2 \; \pa{1 + \gd_5} \pa{1 + \gd_6} \\
t_6 &= \pa{t_4 \; t_1 \; \pa{1 + \gd_7} + \gd \red x} \pa{1 + \gd_8} \\
t_7 &= \pa{\round{s_k} \; t_6 \; \pa{1 + \gd_9} + t_5} \pa{1 + \gd_{10}} \\
t_8 &= \pa{t_0 \; \gd_0 \; \pa{1 + \gd_{11}} + t_7} \pa{1 + \gd_{12}}
\end{alignedat}
\right.
\]
The relative error of the entire computation is:
\[
r\of{\red x, \gd \red x} \DefineAs \frac{y + \gd y}{\sin\of{\red x + \gd \red x + \gz_0 \; \red x}} - 1
\]
As explained above (see \nameref{secerroranalysissin}) the expression for $r$ must be rewritten so that $\gd_0$ only appears once.  In this case the $\gd_0$ term is:
\[
\pa{h \round{s_k} - \round{c_k}} \pa{1 - \pa{1 + \gd_{11}} \pa{1 + \gd_{12}}}
\]
and a \textit{Mathematica} computation yields $\abs{r\of{\red x, \gd \red x}} < 2^{-69.217}$.  Again, the largest errors come from table entries that have many ones after their accurate zeroes (or many zeroes after their accurate ones).  The rounding test must be done with $e = \texttt{0x1.0001'B838'5D8B'6p0}$.

As mentioned above (see \nameref{overview}), in these steps we do not compute the terms $h^n \; \gd \red x$ for $n > 1$.  The largest such term is for $n = 2$ and the relative error that it induces is:
\[
\frac{1}{2}\frac{h^2 \; \gd \red x \; \sin x_k}{\cos\of{\red x + \gd \red x}}
\]
This function reaches its extrema on the corners of the trapezoidal domain and is found to be smaller than $2^{-74.348}$ for all $k$, so its contribution to the overall relative error would be very small.  (The term for $n = 1$, on the other hand, induces an error of the order of $2^{-63.346}$ so it must be computed.)

\end{document}
